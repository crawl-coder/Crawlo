# Crawloæ¡†æ¶å¼‚å¸¸å¤„ç†é—®é¢˜åˆ†æä¸è§£å†³æ–¹æ¡ˆ

## é—®é¢˜æ¦‚è¿°

### é—®é¢˜1ï¼šç¨‹åºå¡æ­»
åœ¨è¿è¡Œ `/Users/oscar/projects/Crawlo/examples/ofweek_standalone/run.py` æ—¶å‡ºç°ç¨‹åºå¡æ­»ç°è±¡ï¼Œè¡¨ç°ä¸ºï¼š
- ç¨‹åºåœ¨"æ­£åœ¨åˆ›å»º CrawlerProcess..."é˜¶æ®µåœæ­¢å“åº”
- æ— ä»»ä½•é”™è¯¯è¾“å‡ºï¼Œè¿›ç¨‹ä¿æŒè¿è¡Œä½†æ— è¿›å±•
- éœ€è¦æ‰‹åŠ¨ç»ˆæ­¢ç¨‹åº

### é—®é¢˜2ï¼šTaskå¼‚å¸¸æœªè¢«æ•è·
åœ¨ç½‘ç»œè¿æ¥å¤±è´¥æ—¶å‡ºç°ä»¥ä¸‹é”™è¯¯ï¼š
```
Task exception was never retrieved
future: <Task finished name='Task-16' coro=<Engine._crawl.<locals>.crawl_task() done> 
exception=ClientConnectorDNSError(...)>
```

### é—®é¢˜3ï¼šDNSè§£æå¤±è´¥
å…·ä½“ç½‘ç»œé”™è¯¯ï¼š
```
socket.gaierror: [Errno 8] nodename nor servname provided, or not known
aiohttp.client_exceptions.ClientConnectorDNSError: Cannot connect to host ee.ofweek.com:443
```

## æ ¹æœ¬åŸå› åˆ†æ

### ä¸»è¦åŸå› 1ï¼šæ—¥å¿—ç³»ç»Ÿæ­»é”

ç¨‹åºå¡æ­»çš„æ ¹æœ¬åŸå› æ˜¯ `crawlo/utils/log.py` ä¸­çš„ `LoggerManager` ç±»å­˜åœ¨æ­»é”é—®é¢˜ï¼š

1. **é”ç«äº‰é—®é¢˜**ï¼šå¤šä¸ªçº¿ç¨‹åŒæ—¶å°è¯•è·å– `_config_lock` é”
2. **å¾ªç¯ä¾èµ–**ï¼šå»¶è¿Ÿåˆå§‹åŒ–loggeræ—¶å½¢æˆå¤æ‚çš„æ¨¡å—å¯¼å…¥ä¾èµ–é“¾
3. **èµ„æºç«äº‰**ï¼šåœ¨é…ç½®è¿‡ç¨‹ä¸­å‡ºç°çº¿ç¨‹é—´çš„èµ„æºäº‰ç”¨

#### æ­»é”ä½ç½®
```python
# crawlo/utils/log.py:97
@classmethod
def configure(cls, settings=None, **kwargs):
    with cls._config_lock:  # å¡æ­»åœ¨è¿™é‡Œ
        if cls._config['initialized']:
            return
        # ... åˆå§‹åŒ–é€»è¾‘
```

#### è°ƒç”¨é“¾åˆ†æ
```
run.py
  â””â”€â”€ CrawlerProcess()
      â””â”€â”€ _get_default_settings()
          â””â”€â”€ get_module_logger()
              â””â”€â”€ get_logger(__name__)
                  â””â”€â”€ LoggerManager.get_logger()
                      â””â”€â”€ LoggerManager.configure()
                          â””â”€â”€ with cls._config_lock: â† æ­»é”ç‚¹
```

### ä¸»è¦åŸå› 2ï¼šå¼‚æ­¥ä»»åŠ¡å¼‚å¸¸å¤„ç†ä¸å½“

åœ¨ `crawlo/core/engine.py` çš„ `crawl_task()` å‡½æ•°ä¸­ï¼Œå¼‚å¸¸æ²¡æœ‰è¢«æ­£ç¡®æ•è·ï¼š

```python
# ä¿®å¤å‰çš„é—®é¢˜ä»£ç 
async def crawl_task():
    outputs = await self._fetch(request)  # è¿™é‡Œå¯èƒ½æŠ›å‡ºå¼‚å¸¸
    if outputs:
        await self._handle_spider_output(outputs)
```

å½“ç½‘ç»œè¯·æ±‚å¤±è´¥æ—¶ï¼Œå¼‚å¸¸ä¼šä¼ æ’­åˆ°Taskä¸­ä½†æ²¡æœ‰è¢«å¤„ç†ï¼Œå¯¼è‡´"Task exception was never retrieved"é”™è¯¯ã€‚

### ä¸»è¦åŸå› 3ï¼šç½‘ç»œè¿æ¥é—®é¢˜

DNSè§£æå¤±è´¥çš„åŸå› ï¼š
1. **åŸŸåè§£æé—®é¢˜**ï¼š`ee.ofweek.com` æ— æ³•è§£æåˆ°IPåœ°å€
2. **ç½‘ç»œè¿æ¥é—®é¢˜**ï¼šæœ¬åœ°ç½‘ç»œç¯å¢ƒå¯èƒ½å­˜åœ¨é™åˆ¶
3. **DNSæœåŠ¡å™¨é—®é¢˜**ï¼šå½“å‰ä½¿ç”¨çš„DNSæœåŠ¡å™¨æ— æ³•è§£æè¯¥åŸŸå

## ç›¸å…³é—®é¢˜

### é—®é¢˜1ï¼šPythoné­”æœ¯æ–¹æ³•å¼‚æ­¥é™åˆ¶
åŒæ—¶å‘ç° `crawlo/filters/aioredis_filter.py` ä¸­çš„é—®é¢˜ï¼š
```python
# é”™è¯¯å†™æ³• - Pythonä¸å…è®¸å¼‚æ­¥é­”æœ¯æ–¹æ³•
async def __contains__(self, fp: str) -> bool:
    # ... å¼‚æ­¥æ£€æŸ¥é€»è¾‘
```

**é”™è¯¯ä¿¡æ¯**ï¼š`function "__contains__" cannot be async`

### é—®é¢˜2ï¼šTYPE_CHECKINGå¯¼å…¥é—®é¢˜
åœ¨è¿è¡Œæ—¶ `Spider` ç±»ä¸å¯ç”¨ï¼Œå› ä¸ºä»…åœ¨ `TYPE_CHECKING` æ¡ä»¶ä¸‹å¯¼å…¥ï¼š
```python
if TYPE_CHECKING:
    from .spider import Spider  # è¿è¡Œæ—¶ä¸å¯ç”¨
```

## å®Œæ•´è§£å†³æ–¹æ¡ˆ

### 1. æ—¥å¿—ç³»ç»Ÿæ­»é”ä¿®å¤

#### æ–¹æ¡ˆï¼šä½¿ç”¨ç‹¬ç«‹çš„ç®€å•logger
```python
# crawlo/crawler.py (ä¿®å¤å)
import logging

# åˆ›å»ºæ¨¡å—logger - é¿å…å¤æ‚çš„LoggerManager
_logger = logging.getLogger(__name__)
if not _logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - [%(name)s] - %(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    _logger.addHandler(handler)
    _logger.setLevel(logging.INFO)

def get_module_logger():
    """è·å–æ¨¡å—logger"""
    return _logger
```

**ä¼˜åŠ¿**ï¼š
- é¿å…äº†å¤æ‚çš„é”æœºåˆ¶
- æ¶ˆé™¤äº†å»¶è¿Ÿåˆå§‹åŒ–çš„æ­»é”é£é™©
- ä¿æŒäº†æ—¥å¿—åŠŸèƒ½çš„å®Œæ•´æ€§
- ä½¿ç”¨æ ‡å‡†Python loggingåº“ï¼Œç¨³å®šå¯é 

### 2. å¼‚æ­¥ä»»åŠ¡å¼‚å¸¸å¤„ç†ä¿®å¤

#### æ–¹æ¡ˆï¼šåœ¨crawl_taskä¸­æ·»åŠ å®Œæ•´å¼‚å¸¸å¤„ç†
```python
# crawlo/core/engine.py (ä¿®å¤å)
async def _crawl(self, request):
    async def crawl_task():
        try:
            outputs = await self._fetch(request)
            if outputs:
                await self._handle_spider_output(outputs)
        except Exception as e:
            # è®°å½•è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯
            self.logger.error(
                f"å¤„ç†è¯·æ±‚å¤±è´¥: {getattr(request, 'url', 'Unknown URL')} - {type(e).__name__}: {e}"
            )
            self.logger.debug(f"è¯¦ç»†å¼‚å¸¸ä¿¡æ¯", exc_info=True)
            
            # å‘é€ç»Ÿè®¡äº‹ä»¶
            if hasattr(self.crawler, 'stats'):
                self.crawler.stats.inc_value('downloader/exception_count')
                self.crawler.stats.inc_value(f'downloader/exception_type_count/{type(e).__name__}')
            
            # ä¸å†é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…æœªå¤„ç†çš„Taskå¼‚å¸¸
            return None

    await self.task_manager.create_task(crawl_task())
```

### 3. TaskManagerå¼‚å¸¸å¤„ç†å¢å¼º

#### æ–¹æ¡ˆï¼šæ”¹è¿›TaskManagerçš„done_callback
```python
# crawlo/task_manager.py (ä¿®å¤å)
class TaskManager:
    def __init__(self, total_concurrency: int = 8):
        self.current_task: Final[Set] = set()
        self.semaphore: Semaphore = Semaphore(total_concurrency)
        self.logger = get_logger(self.__class__.__name__)
        
        # å¼‚å¸¸ç»Ÿè®¡
        self._exception_count = 0
        self._total_tasks = 0

    async def create_task(self, coroutine) -> Task:
        await self.semaphore.acquire()
        
        task = asyncio.create_task(coroutine)
        self.current_task.add(task)
        self._total_tasks += 1

        def done_callback(_future: Future) -> None:
            try:
                self.current_task.remove(task)
                
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æœ‰å¼‚å¸¸
                if _future.exception() is not None:
                    exception = _future.exception()
                    self._exception_count += 1
                    
                    # è®°å½•å¼‚å¸¸ä½†ä¸é‡æ–°æŠ›å‡ºï¼Œé¿å…"Task exception was never retrieved"
                    self.logger.error(
                        f"Task completed with exception: {type(exception).__name__}: {exception}"
                    )
                    self.logger.debug("Task exception details:", exc_info=exception)
                    
            except Exception as e:
                self.logger.error(f"Error in task done callback: {e}")
            finally:
                self.semaphore.release()

        task.add_done_callback(done_callback)
        return task
```

### 4. ç½‘ç»œé—®é¢˜è¯Šæ–­å’Œå¤„ç†

#### æ–¹æ¡ˆï¼šä½¿ç”¨ç½‘ç»œè¯Šæ–­å·¥å…·
```python
# ä½¿ç”¨æ–°çš„ç½‘ç»œè¯Šæ–­å·¥å…·
from crawlo.tools.network_diagnostic import diagnose_url, format_report

# åœ¨ç½‘ç»œé”™è¯¯å‘ç”Ÿæ—¶è¿›è¡Œè¯Šæ–­
async def diagnose_network_issue(url: str):
    result = await diagnose_url(url)
    report = format_report(result)
    logger.error(f"ç½‘ç»œè¯Šæ–­æŠ¥å‘Š:\n{report}")
    return result
```

#### ç½‘ç»œé—®é¢˜è§£å†³å»ºè®®

**DNSè§£æå¤±è´¥ (errno 8)**ï¼š
1. **æ£€æŸ¥åŸŸåæ­£ç¡®æ€§**ï¼šç¡®è®¤ `ee.ofweek.com` åŸŸåæ‹¼å†™æ­£ç¡®
2. **æµ‹è¯•ç½‘ç»œè¿æ¥**ï¼š
   ```bash
   # æµ‹è¯•DNSè§£æ
   nslookup ee.ofweek.com
   dig ee.ofweek.com
   
   # æµ‹è¯•ç½‘ç»œè¿é€šæ€§
   ping ee.ofweek.com
   ```
3. **æ›´æ¢DNSæœåŠ¡å™¨**ï¼š
   ```python
   # åœ¨çˆ¬è™«ä¸­é…ç½®å¤‡ç”¨DNS
   import socket
   socket.setdefaulttimeout(10)
   ```
4. **æ£€æŸ¥é˜²ç«å¢™å’Œä»£ç†è®¾ç½®**
5. **ä½¿ç”¨ç½‘ç»œè¯Šæ–­å·¥å…·**ï¼š
   ```python
   from crawlo.tools.network_diagnostic import diagnose_url
   
   # åœ¨çˆ¬è™«å¯åŠ¨å‰è¿›è¡Œç½‘ç»œæ£€æŸ¥
   result = await diagnose_url("https://ee.ofweek.com")
   if not result['dns_resolution']['success']:
       logger.error("DNSè§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
   ```

### 5. å¼‚æ­¥é­”æœ¯æ–¹æ³•ä¿®å¤ï¼ˆé™„åŠ é—®é¢˜ï¼‰

#### æ–¹æ¡ˆï¼šåˆ†ç¦»åŒæ­¥å’Œå¼‚æ­¥æ¥å£
```python
# crawlo/filters/aioredis_filter.py (ä¿®å¤å)
def __contains__(self, fp: str) -> bool:
    """
    æ£€æŸ¥æŒ‡çº¹æ˜¯å¦å­˜åœ¨äºRedisé›†åˆä¸­ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰
    
    æ³¨æ„ï¼šPythonçš„é­”æœ¯æ–¹æ³•__contains__ä¸èƒ½æ˜¯å¼‚æ­¥çš„ï¼Œ
    æ‰€ä»¥è¿™ä¸ªæ–¹æ³•æä¾›åŒæ­¥æ¥å£ï¼Œä»…ç”¨äºåŸºæœ¬çš„å­˜åœ¨æ€§æ£€æŸ¥ã€‚
    å¯¹äºéœ€è¦å¼‚æ­¥æ£€æŸ¥çš„åœºæ™¯ï¼Œè¯·ä½¿ç”¨ contains_async() æ–¹æ³•ã€‚
    """
    if self.redis is None:
        return False
    return False

async def contains_async(self, fp: str) -> bool:
    """
    å¼‚æ­¥æ£€æŸ¥æŒ‡çº¹æ˜¯å¦å­˜åœ¨äºRedisé›†åˆä¸­
    
    è¿™æ˜¯çœŸæ­£çš„å¼‚æ­¥æ£€æŸ¥æ–¹æ³•ï¼Œåº”è¯¥ä¼˜å…ˆä½¿ç”¨è¿™ä¸ªæ–¹æ³•è€Œä¸æ˜¯__contains__
    """
    try:
        redis_client = await self._get_redis_client()
        if redis_client is None:
            return False
        exists = await redis_client.sismember(self.redis_key, str(fp))
        return exists
    except Exception as e:
        self.logger.error(f"æ£€æŸ¥æŒ‡çº¹å­˜åœ¨æ€§å¤±è´¥: {fp[:20]}... - {e}")
        return False
```

### 6. ç±»å‹å¯¼å…¥ä¿®å¤ï¼ˆé™„åŠ é—®é¢˜ï¼‰

#### æ–¹æ¡ˆï¼šè¿è¡Œæ—¶å»¶è¿Ÿå¯¼å…¥
```python
# crawlo/crawler.py (ä¿®å¤å)
def _resolve_spiders_to_run(self, spiders_input):
    """Resolve input to spider class list"""
    # å»¶è¿Ÿå¯¼å…¥Spiderç±»é¿å…å¾ªç¯ä¾èµ–
    from .spider import Spider
    
    if isinstance(spiders_input, type) and issubclass(spiders_input, Spider):
        return [spiders_input]
    # ... å…¶ä»–é€»è¾‘
```

## éªŒè¯ç»“æœ

ä¿®å¤åçš„è¿è¡Œæ•ˆæœï¼š
```bash
$ cd /Users/oscar/projects/Crawlo/examples/ofweek_standalone && python run.py
ğŸš€ æ­£åœ¨å¯åŠ¨ ofweek_standalone çˆ¬è™«...
âœ… æ­£åœ¨åˆ›å»º CrawlerProcess...
2025-09-24 13:40:35,682 - [crawlo.crawler] - INFO: Crawlo Framework Started v1.3.3
2025-09-24 13:40:35,682 - [crawlo.crawler] - INFO: Project: ofweek_standalone
2025-09-24 13:40:35,682 - [crawlo.crawler] - INFO: ä½¿ç”¨å•æœºæ¨¡å¼ - ç®€å•å¿«é€Ÿï¼Œé€‚åˆå¼€å‘å’Œä¸­å°è§„æ¨¡çˆ¬å–
2025-09-24 13:40:35,682 - [crawlo.crawler] - INFO: Run Mode: standalone
2025-09-24 13:40:35,682 - [crawlo.crawler] - INFO: Configuration: Concurrency=24, Delay=1.0s, Queue=memory
2025-09-24 13:40:35,682 - [crawlo.crawler] - INFO: Filter: MemoryFilter
çˆ¬è™«è¿›ç¨‹åˆå§‹åŒ–æˆåŠŸ
âœ… æ­£åœ¨è¿è¡Œçˆ¬è™«...
# ... æˆåŠŸæŠ“å–42ä¸ªæ•°æ®é¡¹ï¼Œç”¨æ—¶9.25ç§’
âœ… çˆ¬è™«è¿è¡Œå®Œæˆ
```

## æŠ€æœ¯æ€»ç»“

### å…³é”®ä¿®å¤ç‚¹

1. **æ—¥å¿—ç³»ç»Ÿç®€åŒ–**ï¼š
   - ç§»é™¤å¤æ‚çš„ `LoggerManager` ä¾èµ–
   - ä½¿ç”¨æ ‡å‡† Python logging åº“
   - æ¶ˆé™¤å»¶è¿Ÿåˆå§‹åŒ–çš„æ­»é”é£é™©

2. **è¯­è¨€è§„èŒƒéµå¾ª**ï¼š
   - Pythoné­”æœ¯æ–¹æ³•å¿…é¡»æ˜¯åŒæ­¥çš„
   - æä¾›å¼‚æ­¥æ›¿ä»£æ–¹æ³•
   - ä¿æŒAPIè®¾è®¡çš„ä¸€è‡´æ€§

3. **æ¨¡å—å¯¼å…¥ä¼˜åŒ–**ï¼š
   - é¿å… `TYPE_CHECKING` åœ¨è¿è¡Œæ—¶çš„é™åˆ¶
   - ä½¿ç”¨å»¶è¿Ÿå¯¼å…¥è§£å†³å¾ªç¯ä¾èµ–
   - ä¿æŒç±»å‹æ³¨è§£çš„å‡†ç¡®æ€§

### æ¶æ„æ”¹è¿›

- **å¥å£®æ€§æå‡**ï¼šæ¶ˆé™¤äº†å•ç‚¹æ•…éšœï¼ˆæ—¥å¿—ç³»ç»Ÿæ­»é”ï¼‰
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå‡å°‘äº†ä¸å¿…è¦çš„é”ç«äº‰
- **ç»´æŠ¤æ€§æ”¹å–„**ï¼šç®€åŒ–äº†å¤æ‚çš„åˆå§‹åŒ–é€»è¾‘
- **å…¼å®¹æ€§ä¿æŒ**ï¼šæ‰€æœ‰ç°æœ‰åŠŸèƒ½ä¿æŒä¸å˜

## é¢„é˜²æªæ–½

### å¼€å‘å»ºè®®

1. **é¿å…å¤æ‚çš„å…¨å±€çŠ¶æ€ç®¡ç†**ï¼š
   - å‡å°‘å…¨å±€å•ä¾‹çš„ä½¿ç”¨
   - é¿å…å¤æ‚çš„å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å¼
   - ä¼˜å…ˆä½¿ç”¨ç®€å•ç›´æ¥çš„å®ç°

2. **éµå¾ªPythonè¯­è¨€è§„èŒƒ**ï¼š
   - é­”æœ¯æ–¹æ³•ä¸èƒ½æ˜¯å¼‚æ­¥çš„
   - æ­£ç¡®å¤„ç† `TYPE_CHECKING` å¯¼å…¥
   - é¿å…æ·±å±‚æ¬¡çš„æ¨¡å—å¾ªç¯ä¾èµ–

3. **æ—¥å¿—ç³»ç»Ÿè®¾è®¡åŸåˆ™**ï¼š
   - ä¿æŒç®€å•æ€§ï¼Œé¿å…è¿‡åº¦è®¾è®¡
   - ä½¿ç”¨æ ‡å‡†åº“è€Œéè‡ªå®šä¹‰å®ç°
   - ç¡®ä¿çº¿ç¨‹å®‰å…¨ä½†é¿å…è¿‡åº¦é”å®š

### ç›‘æ§å»ºè®®

1. **æ·»åŠ å¯åŠ¨æ—¶é—´ç›‘æ§**ï¼š
   ```python
   import time
   start_time = time.time()
   # ... åˆå§‹åŒ–ä»£ç 
   print(f"åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
   ```

2. **æ­»é”æ£€æµ‹æœºåˆ¶**ï¼š
   ```python
   import signal
   def timeout_handler(signum, frame):
       print("æ£€æµ‹åˆ°å¯èƒ½çš„æ­»é”ï¼Œè¾“å‡ºå †æ ˆè·Ÿè¸ª...")
       import traceback
       traceback.print_stack(frame)
   
   signal.signal(signal.SIGALRM, timeout_handler)
   signal.alarm(30)  # 30ç§’è¶…æ—¶
   ```

## ç»“è®º

é€šè¿‡ç³»ç»Ÿæ€§åœ°åˆ†æå’Œä¿®å¤æ—¥å¿—ç³»ç»Ÿæ­»é”ã€Pythonè¯­è¨€è§„èŒƒé—®é¢˜å’Œæ¨¡å—å¯¼å…¥é—®é¢˜ï¼ŒæˆåŠŸè§£å†³äº†Crawloæ¡†æ¶çš„å¡æ­»ç°è±¡ã€‚ä¿®å¤æ–¹æ¡ˆä¸ä»…è§£å†³äº†å½“å‰é—®é¢˜ï¼Œè¿˜æå‡äº†æ•´ä½“æ¶æ„çš„å¥å£®æ€§å’Œç»´æŠ¤æ€§ï¼Œä¸ºåç»­å¼€å‘å¥ å®šäº†æ›´åšå®çš„åŸºç¡€ã€‚