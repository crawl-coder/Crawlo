# Crawlo æ¡†æ¶é…ç½®ä¼˜å…ˆçº§è¯¦è§£

## ğŸ“‹ é…ç½®æ¥æºæ¦‚è§ˆ

Crawlo æ¡†æ¶çš„é…ç½®æ¥æºæœ‰ä»¥ä¸‹å‡ ä¸ªå±‚çº§ï¼ˆä»ä½åˆ°é«˜ï¼‰ï¼š

1. **æ¡†æ¶é»˜è®¤é…ç½®** (`default_settings.py`)
2. **ç”¨æˆ·é¡¹ç›®é…ç½®** (é¡¹ç›®çš„ `settings.py`)
3. **Spider è‡ªå®šä¹‰é…ç½®** (Spider ç±»çš„ `custom_settings`)
4. **è¿è¡Œæ—¶é…ç½®** (`CrawlerProcess`/`Crawler` ä¼ å…¥çš„ `settings` å‚æ•°)
5. **ç¯å¢ƒå˜é‡** (ä»¥ `CRAWLO_` å¼€å¤´çš„ç¯å¢ƒå˜é‡)

---

## ğŸ¯ é…ç½®åŠ è½½æµç¨‹ä¸ä¼˜å…ˆçº§

### ç¬¬ä¸€æ­¥ï¼šæ¡†æ¶åˆå§‹åŒ– - åŠ è½½é»˜è®¤é…ç½®

**æ–‡ä»¶ä½ç½®**: `/Users/oscar/projects/Crawlo/crawlo/settings/default_settings.py`

**ä½œç”¨**: æä¾›æ‰€æœ‰é…ç½®é¡¹çš„é»˜è®¤å€¼

```python
# default_settings.py ç¤ºä¾‹
PROJECT_NAME = 'crawlo'  # é»˜è®¤é¡¹ç›®å
CONCURRENCY = 8          # é»˜è®¤å¹¶å‘æ•°
DOWNLOAD_DELAY = 0.5     # é»˜è®¤ä¸‹è½½å»¶è¿Ÿ
REDIS_HOST = '127.0.0.1' # é»˜è®¤Redisä¸»æœº
MIDDLEWARES = [...]      # é»˜è®¤ä¸­é—´ä»¶åˆ—è¡¨
PIPELINES = [...]        # é»˜è®¤ç®¡é“åˆ—è¡¨
```

**ä¼˜å…ˆçº§**: â­ (æœ€ä½)

---

### ç¬¬äºŒæ­¥ï¼šè¯»å–ç¯å¢ƒå˜é‡

**æ–‡ä»¶ä½ç½®**: `/Users/oscar/projects/Crawlo/crawlo/utils/config_manager.py`

**å…³é”®ç±»**: `EnvConfigManager`

```python
class EnvConfigManager:
    @staticmethod
    def get_redis_config() -> dict:
        return {
            'REDIS_HOST': EnvConfigManager.get_env_var('CRAWLO_REDIS_HOST', '127.0.0.1', str),
            'REDIS_PORT': EnvConfigManager.get_env_var('CRAWLO_REDIS_PORT', 6379, int),
            # ...
        }
    
    @staticmethod
    def get_runtime_config() -> dict:
        return {
            'CRAWLO_MODE': EnvConfigManager.get_env_var('CRAWLO_MODE', 'standalone', str),
            'PROJECT_NAME': EnvConfigManager.get_env_var('CRAWLO_PROJECT_NAME', 'crawlo', str),
            'CONCURRENCY': EnvConfigManager.get_env_var('CRAWLO_CONCURRENCY', 8, int),
        }
```

**ç¯å¢ƒå˜é‡è§„èŒƒ**:
- æ‰€æœ‰ç¯å¢ƒå˜é‡å¿…é¡»ä½¿ç”¨ `CRAWLO_` å‰ç¼€
- æ”¯æŒçš„ç¯å¢ƒå˜é‡ï¼š
  - `CRAWLO_MODE` - è¿è¡Œæ¨¡å¼
  - `CRAWLO_PROJECT_NAME` - é¡¹ç›®åç§°
  - `CRAWLO_CONCURRENCY` - å¹¶å‘æ•°
  - `CRAWLO_REDIS_HOST` - Redisä¸»æœº
  - `CRAWLO_REDIS_PORT` - Redisç«¯å£
  - `CRAWLO_REDIS_PASSWORD` - Rediså¯†ç 
  - `CRAWLO_REDIS_DB` - Redisæ•°æ®åº“

**ä¼˜å…ˆçº§**: â­â­ (åœ¨ `default_settings.py` ä¸­é€šè¿‡ `EnvConfigManager` è¯»å–ï¼Œä¼šè¦†ç›–éƒ¨åˆ†é»˜è®¤å€¼)

**ç¤ºä¾‹**:
```bash
# é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®
export CRAWLO_PROJECT_NAME=myproject
export CRAWLO_CONCURRENCY=16
export CRAWLO_REDIS_HOST=192.168.1.100
```

---

### ç¬¬ä¸‰æ­¥ï¼šåŠ è½½ç”¨æˆ·é¡¹ç›®é…ç½®

**æ–‡ä»¶ä½ç½®**: ç”¨æˆ·é¡¹ç›®çš„ `settings.py`

**åŠ è½½æ—¶æœº**: æ¡†æ¶åˆå§‹åŒ–æ—¶é€šè¿‡ `initialize_framework()` è‡ªåŠ¨åŠ è½½

```python
# ç”¨æˆ·é¡¹ç›®çš„ settings.py
from crawlo.config import CrawloConfig

# æ–¹å¼1: ä½¿ç”¨ CrawloConfigï¼ˆæ¨èï¼‰
config = CrawloConfig.auto(
    project_name='myproject',
    concurrency=16,
    download_delay=1.0
)
locals().update(config.to_dict())

# æ–¹å¼2: ç›´æ¥è®¾ç½®å˜é‡
PROJECT_NAME = 'myproject'
CONCURRENCY = 16
DOWNLOAD_DELAY = 1.0

# è‡ªå®šä¹‰ä¸­é—´ä»¶ï¼ˆä¼šä¸é»˜è®¤ä¸­é—´ä»¶åˆå¹¶ï¼‰
MIDDLEWARES = [
    'myproject.middlewares.MyCustomMiddleware',
]

# è‡ªå®šä¹‰ç®¡é“ï¼ˆä¼šä¸é»˜è®¤ç®¡é“åˆå¹¶ï¼‰
PIPELINES = [
    'myproject.pipelines.MyCustomPipeline',
]
```

**åˆå¹¶ç­–ç•¥**:

**æ–‡ä»¶ä½ç½®**: `/Users/oscar/projects/Crawlo/crawlo/settings/setting_manager.py`

```python
def _merge_config(self, user_config):
    """åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®"""
    # 1. MIDDLEWARES: åˆå¹¶é»˜è®¤å’Œç”¨æˆ·ä¸­é—´ä»¶ï¼Œå»é‡ä½†ä¿æŒé¡ºåº
    if 'MIDDLEWARES' in user_config:
        merged_middlewares = default_middlewares[:]
        for middleware in user_middlewares:
            if middleware not in merged_middlewares:
                merged_middlewares.append(middleware)
    
    # 2. PIPELINES: åˆå¹¶é»˜è®¤å’Œç”¨æˆ·ç®¡é“ï¼Œå»é‡ä½†ä¿æŒé¡ºåº
    # 3. EXTENSIONS: åˆå¹¶é»˜è®¤å’Œç”¨æˆ·æ‰©å±•ï¼Œå»é‡ä½†ä¿æŒé¡ºåº
    
    # 4. å…¶ä»–é…ç½®: ç›´æ¥è¦†ç›–é»˜è®¤å€¼
    for key, value in user_config.items():
        if key not in ['MIDDLEWARES', 'PIPELINES', 'EXTENSIONS']:
            self.attributes[key] = value
```

**ä¼˜å…ˆçº§**: â­â­â­ (è¦†ç›–é»˜è®¤é…ç½®å’Œç¯å¢ƒå˜é‡)

---

### ç¬¬å››æ­¥ï¼šSpider è‡ªå®šä¹‰é…ç½®

**å®šä¹‰ä½ç½®**: Spider ç±»çš„ `custom_settings` å±æ€§

```python
# åœ¨ Spider ç±»ä¸­å®šä¹‰
class MySpider(Spider):
    name = 'myspider'
    
    custom_settings = {
        'CONCURRENCY': 32,           # è¦†ç›–é¡¹ç›®é…ç½®
        'DOWNLOAD_DELAY': 2.0,       # è¦†ç›–é¡¹ç›®é…ç½®
        'PIPELINES': [               # ä¼šä¸é»˜è®¤ç®¡é“åˆå¹¶
            'myproject.pipelines.SpecialPipeline',
        ]
    }
```

**åº”ç”¨æ—¶æœº**: åœ¨åˆ›å»º `Crawler` å®ä¾‹æ—¶è‡ªåŠ¨åˆå¹¶

**æ–‡ä»¶ä½ç½®**: `/Users/oscar/projects/Crawlo/crawlo/crawler.py`

```python
def _merge_settings(self, additional_settings):
    """åˆå¹¶é…ç½®"""
    if not additional_settings:
        return self._settings
    
    merged = SettingManager()
    
    # 1. å¤åˆ¶åŸºç¡€é…ç½®ï¼ˆæ¥è‡ªé¡¹ç›® settings.pyï¼‰
    if self._settings:
        merged.update_attributes(self._settings.__dict__)
    
    # 2. åº”ç”¨é¢å¤–é…ç½®ï¼ˆSpider çš„ custom_settingsï¼‰
    merged.update_attributes(additional_settings)
    
    return merged
```

**ä¼˜å…ˆçº§**: â­â­â­â­ (è¦†ç›–é¡¹ç›®é…ç½®)

---

### ç¬¬äº”æ­¥ï¼šè¿è¡Œæ—¶é…ç½®

**ä¼ å…¥ä½ç½®**: è°ƒç”¨ `CrawlerProcess.crawl()` æˆ– `Crawler.__init__()` æ—¶ä¼ å…¥

```python
# æ–¹å¼1: é€šè¿‡ CrawlerProcess.crawl()
from crawlo import CrawlerProcess

process = CrawlerProcess()
await process.crawl(
    MySpider,
    settings={
        'CONCURRENCY': 64,        # è¿è¡Œæ—¶è¦†ç›–
        'DOWNLOAD_DELAY': 0.1,    # è¿è¡Œæ—¶è¦†ç›–
    }
)

# æ–¹å¼2: ç›´æ¥åˆ›å»º Crawler
from crawlo import Crawler

crawler = Crawler(
    MySpider,
    settings={
        'CONCURRENCY': 64,
    }
)
```

**åº”ç”¨æ—¶æœº**: åœ¨ `CrawlerProcess._merge_settings()` ä¸­åˆå¹¶

**ä¼˜å…ˆçº§**: â­â­â­â­â­ (æœ€é«˜ï¼Œè¦†ç›–æ‰€æœ‰å…¶ä»–é…ç½®)

---

## ğŸ“Š å®Œæ•´ä¼˜å…ˆçº§æ€»ç»“

ä»**ä½åˆ°é«˜**çš„ä¼˜å…ˆçº§é¡ºåºï¼š

```
1. default_settings.py (æ¡†æ¶é»˜è®¤é…ç½®)
   â†“
2. ç¯å¢ƒå˜é‡ (CRAWLO_* ç¯å¢ƒå˜é‡ï¼Œåœ¨ default_settings.py ä¸­é€šè¿‡ EnvConfigManager è¯»å–)
   â†“
3. ç”¨æˆ· settings.py (é¡¹ç›®é…ç½®æ–‡ä»¶)
   â†“
4. Spider.custom_settings (Spider è‡ªå®šä¹‰é…ç½®)
   â†“
5. è¿è¡Œæ—¶ settings å‚æ•° (crawl() æˆ– Crawler() ä¼ å…¥çš„é…ç½®)
```

**é‡è¦è¯´æ˜**:
- ç¯å¢ƒå˜é‡è™½ç„¶åœ¨ç¬¬2ä½ï¼Œä½†å®ƒåœ¨ `default_settings.py` ä¸­å°±å·²ç»è¢«è¯»å–å’Œåº”ç”¨
- `MIDDLEWARES`ã€`PIPELINES`ã€`EXTENSIONS` é‡‡ç”¨**åˆå¹¶ç­–ç•¥**ï¼Œè€Œä¸æ˜¯ç®€å•è¦†ç›–
- å…¶ä»–æ™®é€šé…ç½®é‡‡ç”¨**è¦†ç›–ç­–ç•¥**ï¼Œåè€…è¦†ç›–å‰è€…

---

## ğŸ” å®é™…æ¡ˆä¾‹åˆ†æ

### æ¡ˆä¾‹ 1: CONCURRENCY é…ç½®

å‡è®¾æœ‰ä»¥ä¸‹é…ç½®æ¥æºï¼š

```python
# 1. default_settings.py
CONCURRENCY = 8  # é»˜è®¤å€¼

# 2. ç¯å¢ƒå˜é‡
# export CRAWLO_CONCURRENCY=12

# 3. ç”¨æˆ· settings.py
CONCURRENCY = 16

# 4. Spider.custom_settings
class MySpider(Spider):
    custom_settings = {
        'CONCURRENCY': 24
    }

# 5. è¿è¡Œæ—¶é…ç½®
await process.crawl(MySpider, settings={'CONCURRENCY': 32})
```

**æœ€ç»ˆç»“æœ**: `CONCURRENCY = 32` (è¿è¡Œæ—¶é…ç½®ä¼˜å…ˆçº§æœ€é«˜)

**é€æ­¥è¦†ç›–è¿‡ç¨‹**:
1. åˆå§‹å€¼: `8` (default_settings.py)
2. ç¯å¢ƒå˜é‡è¦†ç›–: `12` (CRAWLO_CONCURRENCY)
3. é¡¹ç›®é…ç½®è¦†ç›–: `16` (settings.py)
4. Spideré…ç½®è¦†ç›–: `24` (custom_settings)
5. è¿è¡Œæ—¶é…ç½®è¦†ç›–: `32` (crawlå‚æ•°) âœ… æœ€ç»ˆå€¼

---

### æ¡ˆä¾‹ 2: PIPELINES é…ç½®ï¼ˆåˆå¹¶ç­–ç•¥ï¼‰

```python
# 1. default_settings.py
PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',
]

# 2. ç”¨æˆ· settings.py
PIPELINES = [
    'myproject.pipelines.MySQLPipeline',
    'myproject.pipelines.JsonPipeline',
]

# 3. Spider.custom_settings
class MySpider(Spider):
    custom_settings = {
        'PIPELINES': [
            'myproject.pipelines.SpecialPipeline',
        ]
    }
```

**æœ€ç»ˆç»“æœ** (åˆå¹¶å):
```python
PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',  # é»˜è®¤
    'myproject.pipelines.MySQLPipeline',                   # ç”¨æˆ·æ·»åŠ 
    'myproject.pipelines.JsonPipeline',                    # ç”¨æˆ·æ·»åŠ 
    'myproject.pipelines.SpecialPipeline',                 # Spideræ·»åŠ 
]
```

**åˆå¹¶è§„åˆ™**:
- ä¿ç•™é»˜è®¤ç®¡é“
- æŒ‰é¡ºåºæ·»åŠ ç”¨æˆ·ç®¡é“
- å»é‡ï¼ˆåŒä¸€ä¸ªç®¡é“åªä¼šå‡ºç°ä¸€æ¬¡ï¼‰
- ä¿æŒæ·»åŠ é¡ºåº

---

### æ¡ˆä¾‹ 3: Redis é…ç½®ï¼ˆç¯å¢ƒå˜é‡ä¼˜å…ˆï¼‰

```python
# 1. default_settings.py
REDIS_HOST = '127.0.0.1'  # é»˜è®¤å€¼
REDIS_PORT = 6379         # é»˜è®¤å€¼

# ä½†å®é™…ä¸Š default_settings.py ä¼šç«‹å³è¯»å–ç¯å¢ƒå˜é‡ï¼š
redis_config = EnvConfigManager.get_redis_config()
REDIS_HOST = redis_config['REDIS_HOST']  # ä» CRAWLO_REDIS_HOST è¯»å–
REDIS_PORT = redis_config['REDIS_PORT']  # ä» CRAWLO_REDIS_PORT è¯»å–
```

**ç¯å¢ƒå˜é‡è®¾ç½®**:
```bash
export CRAWLO_REDIS_HOST=192.168.1.100
export CRAWLO_REDIS_PORT=7000
```

**æœ€ç»ˆç»“æœ**:
- `REDIS_HOST = '192.168.1.100'` (ç¯å¢ƒå˜é‡)
- `REDIS_PORT = 7000` (ç¯å¢ƒå˜é‡)

**å³ä½¿åœ¨ `settings.py` ä¸­è®¾ç½®**:
```python
# settings.py
REDIS_HOST = 'localhost'  # è¿™ä¼šè¦†ç›–ç¯å¢ƒå˜é‡ï¼
```

**å®é™…ç»“æœ**: `REDIS_HOST = 'localhost'` (é¡¹ç›®é…ç½®è¦†ç›–ç¯å¢ƒå˜é‡)

---

## ğŸ“ æœ€ä½³å®è·µå»ºè®®

### 1. å¼€å‘ç¯å¢ƒé…ç½®

```bash
# .env æˆ–ç¯å¢ƒå˜é‡
export CRAWLO_MODE=standalone
export CRAWLO_PROJECT_NAME=myproject
export CRAWLO_CONCURRENCY=8
```

```python
# settings.pyï¼ˆä¿æŒç®€æ´ï¼‰
from crawlo.config import CrawloConfig

config = CrawloConfig.auto(
    project_name='myproject',  # ä¹Ÿå¯ä»¥ä»ç¯å¢ƒå˜é‡è¯»å–
    concurrency=16,             # å¼€å‘ç¯å¢ƒå¹¶å‘æ•°
)
locals().update(config.to_dict())

PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',  # å¼€å‘æ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
]
```

---

### 2. ç”Ÿäº§ç¯å¢ƒé…ç½®

```bash
# ç”Ÿäº§ç¯å¢ƒå˜é‡
export CRAWLO_MODE=distributed
export CRAWLO_PROJECT_NAME=myproject_prod
export CRAWLO_CONCURRENCY=32
export CRAWLO_REDIS_HOST=redis.production.com
export CRAWLO_REDIS_PORT=6379
export CRAWLO_REDIS_PASSWORD=xxx
```

```python
# settings.pyï¼ˆé€šç”¨é…ç½®ï¼Œä¸éœ€è¦åŒºåˆ†ç¯å¢ƒï¼‰
from crawlo.config import CrawloConfig

# Auto æ¨¡å¼ä¼šè‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡
config = CrawloConfig.auto()
locals().update(config.to_dict())

PIPELINES = [
    'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',
]
```

---

### 3. Spider ç‰¹å®šé…ç½®

```python
class QuickSpider(Spider):
    """å¿«é€Ÿçˆ¬è™«ï¼Œä½¿ç”¨æ›´é«˜å¹¶å‘"""
    name = 'quick'
    
    custom_settings = {
        'CONCURRENCY': 64,       # è¦†ç›–é¡¹ç›®é…ç½®
        'DOWNLOAD_DELAY': 0.1,   # æ›´å¿«çš„è¯·æ±‚é€Ÿåº¦
    }

class SlowSpider(Spider):
    """æ…¢é€Ÿçˆ¬è™«ï¼Œé¿å…è¢«å°"""
    name = 'slow'
    
    custom_settings = {
        'CONCURRENCY': 2,        # é™ä½å¹¶å‘
        'DOWNLOAD_DELAY': 5.0,   # å¢åŠ å»¶è¿Ÿ
    }
```

---

### 4. ä¸´æ—¶è°ƒè¯•é…ç½®

```python
# ä¸´æ—¶æµ‹è¯•é«˜å¹¶å‘
from crawlo import CrawlerProcess

process = CrawlerProcess()
await process.crawl(
    MySpider,
    settings={
        'CONCURRENCY': 128,      # ä¸´æ—¶æµ‹è¯•æé™å¹¶å‘
        'LOG_LEVEL': 'DEBUG',    # ä¸´æ—¶å¼€å¯è°ƒè¯•æ—¥å¿—
    }
)
```

---

## âš ï¸ å¸¸è§é™·é˜±ä¸æ³¨æ„äº‹é¡¹

### é™·é˜± 1: ç¯å¢ƒå˜é‡è¢«é¡¹ç›®é…ç½®è¦†ç›–

```python
# âŒ é”™è¯¯ç¤ºä¾‹
# ç¯å¢ƒå˜é‡è®¾ç½®
export CRAWLO_REDIS_HOST=192.168.1.100

# settings.py ä¸­åˆè®¾ç½®äº†
REDIS_HOST = 'localhost'  # è¿™ä¼šè¦†ç›–ç¯å¢ƒå˜é‡ï¼

# ç»“æœ: REDIS_HOST = 'localhost' (é¡¹ç›®é…ç½®ä¼˜å…ˆ)
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# âœ… æ­£ç¡®ç¤ºä¾‹
# å¦‚æœè¦ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œsettings.py ä¸­ä¸è¦é‡å¤è®¾ç½®
# æˆ–è€…ä½¿ç”¨ CrawloConfig.auto() è‡ªåŠ¨å¤„ç†
config = CrawloConfig.auto()  # ä¼šæ­£ç¡®è¯»å–ç¯å¢ƒå˜é‡
locals().update(config.to_dict())
```

---

### é™·é˜± 2: ä¸­é—´ä»¶/ç®¡é“é…ç½®æ··æ·†

```python
# âŒ é”™è¯¯ç†è§£
# æˆ‘åœ¨ settings.py ä¸­è®¾ç½®äº† PIPELINESï¼Œé»˜è®¤ç®¡é“ä¼šè¢«æ¸…ç©ºå—ï¼Ÿ

# settings.py
PIPELINES = [
    'myproject.pipelines.MySQLPipeline',
]

# å®é™…ç»“æœï¼ˆä¼šä¿ç•™é»˜è®¤ç®¡é“ï¼‰:
PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',  # é»˜è®¤ç®¡é“ä¿ç•™
    'myproject.pipelines.MySQLPipeline',                   # ç”¨æˆ·ç®¡é“è¿½åŠ 
]
```

**å¦‚æœæƒ³å®Œå…¨æ›¿æ¢**:
```python
# âœ… æ˜ç¡®æ¸…ç©ºé»˜è®¤é…ç½®
PIPELINES = []  # å…ˆæ¸…ç©º
PIPELINES.append('myproject.pipelines.MySQLPipeline')
```

---

### é™·é˜± 3: custom_settings ä¸ç”Ÿæ•ˆ

```python
# âŒ é”™è¯¯ç¤ºä¾‹
class MySpider(Spider):
    name = 'myspider'
    
    # æ³¨æ„: è¿™æ˜¯ç±»å±æ€§ï¼Œéœ€è¦æ˜¯å­—å…¸
    custom_settings = {
        'CONCURRENCY': 32,
    }

# ä½†å¦‚æœä½ è¿™æ ·å†™ï¼š
class MySpider(Spider):
    def custom_settings(self):  # âŒ é”™è¯¯ï¼è¿™æ˜¯æ–¹æ³•ï¼Œä¸æ˜¯å±æ€§
        return {'CONCURRENCY': 32}
```

**æ­£ç¡®æ–¹å¼**:
```python
# âœ… ç±»å±æ€§ï¼ˆå­—å…¸ï¼‰
class MySpider(Spider):
    custom_settings = {
        'CONCURRENCY': 32,
    }

# âœ… æˆ–è€…ä½¿ç”¨ç±»æ–¹æ³•ï¼ˆä¸æ¨èï¼Œé™¤ééœ€è¦åŠ¨æ€ç”Ÿæˆï¼‰
class MySpider(Spider):
    @classmethod
    def get_custom_settings(cls):
        return {'CONCURRENCY': 32}
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [`default_settings.py`](file:///Users/oscar/projects/Crawlo/crawlo/settings/default_settings.py) - æ¡†æ¶é»˜è®¤é…ç½®
- [`setting_manager.py`](file:///Users/oscar/projects/Crawlo/crawlo/settings/setting_manager.py) - é…ç½®ç®¡ç†å™¨
- [`config_manager.py`](file:///Users/oscar/projects/Crawlo/crawlo/utils/config_manager.py) - ç¯å¢ƒå˜é‡ç®¡ç†
- [`crawler.py`](file:///Users/oscar/projects/Crawlo/crawlo/crawler.py) - Crawleré…ç½®åˆå¹¶é€»è¾‘
- [`config.py`](file:///Users/oscar/projects/Crawlo/crawlo/config.py) - CrawloConfigå·¥å…·ç±»

---
