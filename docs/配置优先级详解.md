# Crawlo 框架配置优先级详解

## 📋 配置来源概览

Crawlo 框架的配置来源有以下几个层级（从低到高）：

1. **框架默认配置** (`default_settings.py`)
2. **用户项目配置** (项目的 `settings.py`)
3. **Spider 自定义配置** (Spider 类的 `custom_settings`)
4. **运行时配置** (`CrawlerProcess`/`Crawler` 传入的 `settings` 参数)
5. **环境变量** (以 `CRAWLO_` 开头的环境变量)

---

## 🎯 配置加载流程与优先级

### 第一步：框架初始化 - 加载默认配置

**文件位置**: `/Users/oscar/projects/Crawlo/crawlo/settings/default_settings.py`

**作用**: 提供所有配置项的默认值

```python
# default_settings.py 示例
PROJECT_NAME = 'crawlo'  # 默认项目名
CONCURRENCY = 8          # 默认并发数
DOWNLOAD_DELAY = 0.5     # 默认下载延迟
REDIS_HOST = '127.0.0.1' # 默认Redis主机
MIDDLEWARES = [...]      # 默认中间件列表
PIPELINES = [...]        # 默认管道列表
```

**优先级**: ⭐ (最低)

---

### 第二步：读取环境变量

**文件位置**: `/Users/oscar/projects/Crawlo/crawlo/utils/config_manager.py`

**关键类**: `EnvConfigManager`

```python
class EnvConfigManager:
    @staticmethod
    def get_redis_config() -> dict:
        return {
            'REDIS_HOST': EnvConfigManager.get_env_var('CRAWLO_REDIS_HOST', '127.0.0.1', str),
            'REDIS_PORT': EnvConfigManager.get_env_var('CRAWLO_REDIS_PORT', 6379, int),
            # ...
        }
    
    @staticmethod
    def get_runtime_config() -> dict:
        return {
            'CRAWLO_MODE': EnvConfigManager.get_env_var('CRAWLO_MODE', 'standalone', str),
            'PROJECT_NAME': EnvConfigManager.get_env_var('CRAWLO_PROJECT_NAME', 'crawlo', str),
            'CONCURRENCY': EnvConfigManager.get_env_var('CRAWLO_CONCURRENCY', 8, int),
        }
```

**环境变量规范**:
- 所有环境变量必须使用 `CRAWLO_` 前缀
- 支持的环境变量：
  - `CRAWLO_MODE` - 运行模式
  - `CRAWLO_PROJECT_NAME` - 项目名称
  - `CRAWLO_CONCURRENCY` - 并发数
  - `CRAWLO_REDIS_HOST` - Redis主机
  - `CRAWLO_REDIS_PORT` - Redis端口
  - `CRAWLO_REDIS_PASSWORD` - Redis密码
  - `CRAWLO_REDIS_DB` - Redis数据库

**优先级**: ⭐⭐ (在 `default_settings.py` 中通过 `EnvConfigManager` 读取，会覆盖部分默认值)

**示例**:
```bash
# 通过环境变量配置
export CRAWLO_PROJECT_NAME=myproject
export CRAWLO_CONCURRENCY=16
export CRAWLO_REDIS_HOST=192.168.1.100
```

---

### 第三步：加载用户项目配置

**文件位置**: 用户项目的 `settings.py`

**加载时机**: 框架初始化时通过 `initialize_framework()` 自动加载

```python
# 用户项目的 settings.py
from crawlo.config import CrawloConfig

# 方式1: 使用 CrawloConfig（推荐）
config = CrawloConfig.auto(
    project_name='myproject',
    concurrency=16,
    download_delay=1.0
)
locals().update(config.to_dict())

# 方式2: 直接设置变量
PROJECT_NAME = 'myproject'
CONCURRENCY = 16
DOWNLOAD_DELAY = 1.0

# 自定义中间件（会与默认中间件合并）
MIDDLEWARES = [
    'myproject.middlewares.MyCustomMiddleware',
]

# 自定义管道（会与默认管道合并）
PIPELINES = [
    'myproject.pipelines.MyCustomPipeline',
]
```

**合并策略**:

**文件位置**: `/Users/oscar/projects/Crawlo/crawlo/settings/setting_manager.py`

```python
def _merge_config(self, user_config):
    """合并默认配置和用户配置"""
    # 1. MIDDLEWARES: 合并默认和用户中间件，去重但保持顺序
    if 'MIDDLEWARES' in user_config:
        merged_middlewares = default_middlewares[:]
        for middleware in user_middlewares:
            if middleware not in merged_middlewares:
                merged_middlewares.append(middleware)
    
    # 2. PIPELINES: 合并默认和用户管道，去重但保持顺序
    # 3. EXTENSIONS: 合并默认和用户扩展，去重但保持顺序
    
    # 4. 其他配置: 直接覆盖默认值
    for key, value in user_config.items():
        if key not in ['MIDDLEWARES', 'PIPELINES', 'EXTENSIONS']:
            self.attributes[key] = value
```

**优先级**: ⭐⭐⭐ (覆盖默认配置和环境变量)

---

### 第四步：Spider 自定义配置

**定义位置**: Spider 类的 `custom_settings` 属性

```python
# 在 Spider 类中定义
class MySpider(Spider):
    name = 'myspider'
    
    custom_settings = {
        'CONCURRENCY': 32,           # 覆盖项目配置
        'DOWNLOAD_DELAY': 2.0,       # 覆盖项目配置
        'PIPELINES': [               # 会与默认管道合并
            'myproject.pipelines.SpecialPipeline',
        ]
    }
```

**应用时机**: 在创建 `Crawler` 实例时自动合并

**文件位置**: `/Users/oscar/projects/Crawlo/crawlo/crawler.py`

```python
def _merge_settings(self, additional_settings):
    """合并配置"""
    if not additional_settings:
        return self._settings
    
    merged = SettingManager()
    
    # 1. 复制基础配置（来自项目 settings.py）
    if self._settings:
        merged.update_attributes(self._settings.__dict__)
    
    # 2. 应用额外配置（Spider 的 custom_settings）
    merged.update_attributes(additional_settings)
    
    return merged
```

**优先级**: ⭐⭐⭐⭐ (覆盖项目配置)

---

### 第五步：运行时配置

**传入位置**: 调用 `CrawlerProcess.crawl()` 或 `Crawler.__init__()` 时传入

```python
# 方式1: 通过 CrawlerProcess.crawl()
from crawlo import CrawlerProcess

process = CrawlerProcess()
await process.crawl(
    MySpider,
    settings={
        'CONCURRENCY': 64,        # 运行时覆盖
        'DOWNLOAD_DELAY': 0.1,    # 运行时覆盖
    }
)

# 方式2: 直接创建 Crawler
from crawlo import Crawler

crawler = Crawler(
    MySpider,
    settings={
        'CONCURRENCY': 64,
    }
)
```

**应用时机**: 在 `CrawlerProcess._merge_settings()` 中合并

**优先级**: ⭐⭐⭐⭐⭐ (最高，覆盖所有其他配置)

---

## 📊 完整优先级总结

从**低到高**的优先级顺序：

```
1. default_settings.py (框架默认配置)
   ↓
2. 环境变量 (CRAWLO_* 环境变量，在 default_settings.py 中通过 EnvConfigManager 读取)
   ↓
3. 用户 settings.py (项目配置文件)
   ↓
4. Spider.custom_settings (Spider 自定义配置)
   ↓
5. 运行时 settings 参数 (crawl() 或 Crawler() 传入的配置)
```

**重要说明**:
- 环境变量虽然在第2位，但它在 `default_settings.py` 中就已经被读取和应用
- `MIDDLEWARES`、`PIPELINES`、`EXTENSIONS` 采用**合并策略**，而不是简单覆盖
- 其他普通配置采用**覆盖策略**，后者覆盖前者

---

## 🔍 实际案例分析

### 案例 1: CONCURRENCY 配置

假设有以下配置来源：

```python
# 1. default_settings.py
CONCURRENCY = 8  # 默认值

# 2. 环境变量
# export CRAWLO_CONCURRENCY=12

# 3. 用户 settings.py
CONCURRENCY = 16

# 4. Spider.custom_settings
class MySpider(Spider):
    custom_settings = {
        'CONCURRENCY': 24
    }

# 5. 运行时配置
await process.crawl(MySpider, settings={'CONCURRENCY': 32})
```

**最终结果**: `CONCURRENCY = 32` (运行时配置优先级最高)

**逐步覆盖过程**:
1. 初始值: `8` (default_settings.py)
2. 环境变量覆盖: `12` (CRAWLO_CONCURRENCY)
3. 项目配置覆盖: `16` (settings.py)
4. Spider配置覆盖: `24` (custom_settings)
5. 运行时配置覆盖: `32` (crawl参数) ✅ 最终值

---

### 案例 2: PIPELINES 配置（合并策略）

```python
# 1. default_settings.py
PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',
]

# 2. 用户 settings.py
PIPELINES = [
    'myproject.pipelines.MySQLPipeline',
    'myproject.pipelines.JsonPipeline',
]

# 3. Spider.custom_settings
class MySpider(Spider):
    custom_settings = {
        'PIPELINES': [
            'myproject.pipelines.SpecialPipeline',
        ]
    }
```

**最终结果** (合并后):
```python
PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',  # 默认
    'myproject.pipelines.MySQLPipeline',                   # 用户添加
    'myproject.pipelines.JsonPipeline',                    # 用户添加
    'myproject.pipelines.SpecialPipeline',                 # Spider添加
]
```

**合并规则**:
- 保留默认管道
- 按顺序添加用户管道
- 去重（同一个管道只会出现一次）
- 保持添加顺序

---

### 案例 3: Redis 配置（环境变量优先）

```python
# 1. default_settings.py
REDIS_HOST = '127.0.0.1'  # 默认值
REDIS_PORT = 6379         # 默认值

# 但实际上 default_settings.py 会立即读取环境变量：
redis_config = EnvConfigManager.get_redis_config()
REDIS_HOST = redis_config['REDIS_HOST']  # 从 CRAWLO_REDIS_HOST 读取
REDIS_PORT = redis_config['REDIS_PORT']  # 从 CRAWLO_REDIS_PORT 读取
```

**环境变量设置**:
```bash
export CRAWLO_REDIS_HOST=192.168.1.100
export CRAWLO_REDIS_PORT=7000
```

**最终结果**:
- `REDIS_HOST = '192.168.1.100'` (环境变量)
- `REDIS_PORT = 7000` (环境变量)

**即使在 `settings.py` 中设置**:
```python
# settings.py
REDIS_HOST = 'localhost'  # 这会覆盖环境变量！
```

**实际结果**: `REDIS_HOST = 'localhost'` (项目配置覆盖环境变量)

---

## 🎓 最佳实践建议

### 1. 开发环境配置

```bash
# .env 或环境变量
export CRAWLO_MODE=standalone
export CRAWLO_PROJECT_NAME=myproject
export CRAWLO_CONCURRENCY=8
```

```python
# settings.py（保持简洁）
from crawlo.config import CrawloConfig

config = CrawloConfig.auto(
    project_name='myproject',  # 也可以从环境变量读取
    concurrency=16,             # 开发环境并发数
)
locals().update(config.to_dict())

PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',  # 开发时输出到控制台
]
```

---

### 2. 生产环境配置

```bash
# 生产环境变量
export CRAWLO_MODE=distributed
export CRAWLO_PROJECT_NAME=myproject_prod
export CRAWLO_CONCURRENCY=32
export CRAWLO_REDIS_HOST=redis.production.com
export CRAWLO_REDIS_PORT=6379
export CRAWLO_REDIS_PASSWORD=xxx
```

```python
# settings.py（通用配置，不需要区分环境）
from crawlo.config import CrawloConfig

# Auto 模式会自动读取环境变量
config = CrawloConfig.auto()
locals().update(config.to_dict())

PIPELINES = [
    'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',
]
```

---

### 3. Spider 特定配置

```python
class QuickSpider(Spider):
    """快速爬虫，使用更高并发"""
    name = 'quick'
    
    custom_settings = {
        'CONCURRENCY': 64,       # 覆盖项目配置
        'DOWNLOAD_DELAY': 0.1,   # 更快的请求速度
    }

class SlowSpider(Spider):
    """慢速爬虫，避免被封"""
    name = 'slow'
    
    custom_settings = {
        'CONCURRENCY': 2,        # 降低并发
        'DOWNLOAD_DELAY': 5.0,   # 增加延迟
    }
```

---

### 4. 临时调试配置

```python
# 临时测试高并发
from crawlo import CrawlerProcess

process = CrawlerProcess()
await process.crawl(
    MySpider,
    settings={
        'CONCURRENCY': 128,      # 临时测试极限并发
        'LOG_LEVEL': 'DEBUG',    # 临时开启调试日志
    }
)
```

---

## ⚠️ 常见陷阱与注意事项

### 陷阱 1: 环境变量被项目配置覆盖

```python
# ❌ 错误示例
# 环境变量设置
export CRAWLO_REDIS_HOST=192.168.1.100

# settings.py 中又设置了
REDIS_HOST = 'localhost'  # 这会覆盖环境变量！

# 结果: REDIS_HOST = 'localhost' (项目配置优先)
```

**解决方案**:
```python
# ✅ 正确示例
# 如果要使用环境变量，settings.py 中不要重复设置
# 或者使用 CrawloConfig.auto() 自动处理
config = CrawloConfig.auto()  # 会正确读取环境变量
locals().update(config.to_dict())
```

---

### 陷阱 2: 中间件/管道配置混淆

```python
# ❌ 错误理解
# 我在 settings.py 中设置了 PIPELINES，默认管道会被清空吗？

# settings.py
PIPELINES = [
    'myproject.pipelines.MySQLPipeline',
]

# 实际结果（会保留默认管道）:
PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',  # 默认管道保留
    'myproject.pipelines.MySQLPipeline',                   # 用户管道追加
]
```

**如果想完全替换**:
```python
# ✅ 明确清空默认配置
PIPELINES = []  # 先清空
PIPELINES.append('myproject.pipelines.MySQLPipeline')
```

---

### 陷阱 3: custom_settings 不生效

```python
# ❌ 错误示例
class MySpider(Spider):
    name = 'myspider'
    
    # 注意: 这是类属性，需要是字典
    custom_settings = {
        'CONCURRENCY': 32,
    }

# 但如果你这样写：
class MySpider(Spider):
    def custom_settings(self):  # ❌ 错误！这是方法，不是属性
        return {'CONCURRENCY': 32}
```

**正确方式**:
```python
# ✅ 类属性（字典）
class MySpider(Spider):
    custom_settings = {
        'CONCURRENCY': 32,
    }

# ✅ 或者使用类方法（不推荐，除非需要动态生成）
class MySpider(Spider):
    @classmethod
    def get_custom_settings(cls):
        return {'CONCURRENCY': 32}
```

---

## 📚 相关文档

- [`default_settings.py`](file:///Users/oscar/projects/Crawlo/crawlo/settings/default_settings.py) - 框架默认配置
- [`setting_manager.py`](file:///Users/oscar/projects/Crawlo/crawlo/settings/setting_manager.py) - 配置管理器
- [`config_manager.py`](file:///Users/oscar/projects/Crawlo/crawlo/utils/config_manager.py) - 环境变量管理
- [`crawler.py`](file:///Users/oscar/projects/Crawlo/crawlo/crawler.py) - Crawler配置合并逻辑
- [`config.py`](file:///Users/oscar/projects/Crawlo/crawlo/config.py) - CrawloConfig工具类

---
