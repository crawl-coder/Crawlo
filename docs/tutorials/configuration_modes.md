# Crawlo é…ç½®æ¨¡å¼å®Œå…¨æŒ‡å—

> æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç» Crawlo æ¡†æ¶çš„ä¸‰ç§é…ç½®æ¨¡å¼ï¼ˆStandaloneã€Distributedã€Autoï¼‰åŠå…¶è¿è¡Œæœºåˆ¶ã€‚

## å¿«é€Ÿå¯¼èˆª

- [ä¸‰ç§æ¨¡å¼å¯¹æ¯”](#ä¸‰ç§æ¨¡å¼å¯¹æ¯”)
- [Standalone æ¨¡å¼](#standalone-æ¨¡å¼)
- [Distributed æ¨¡å¼](#distributed-æ¨¡å¼)
- [Auto æ¨¡å¼](#auto-æ¨¡å¼)
- [é…ç½®æ›´æ–°æœºåˆ¶](#é…ç½®æ›´æ–°æœºåˆ¶)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ä¸‰ç§æ¨¡å¼å¯¹æ¯”

### æ ¸å¿ƒåŒºåˆ«ä¸€è§ˆè¡¨

| é…ç½®é¡¹ | Standalone | Distributed | Auto |
|--------|-----------|-------------|------|
| **RUN_MODE** | `standalone` | `distributed` | `auto` |
| **QUEUE_TYPEï¼ˆé…ç½®é˜¶æ®µï¼‰** | `memory` | `redis` | `auto` |
| **QUEUE_TYPEï¼ˆè¿è¡Œæ—¶ï¼‰** | å›ºå®š `memory` | å¿…é¡» `redis` | è‡ªåŠ¨æ£€æµ‹ |
| **Redis æ£€æµ‹** | âŒ ä¸æ£€æµ‹ | âœ… æ£€æµ‹ï¼ˆå¿…é¡»å¯ç”¨ï¼‰ | âœ… æ£€æµ‹å¹¶å›é€€ |
| **Redis ä¸å¯ç”¨æ—¶** | N/A | ğŸš« **æŠ¥é”™é€€å‡º** | é™çº§åˆ° Memory |
| **é…ç½®è‡ªåŠ¨æ›´æ–°** | âŒ å¦ | âŒ å¦ | âœ… æ˜¯ |
| **è¿‡æ»¤å™¨** | å›ºå®š Memory | å›ºå®š Redis | Redis/Memory |
| **å»é‡ç®¡é“** | å›ºå®š Memory | å›ºå®š Redis | Redis/Memory |
| **é€‚ç”¨åœºæ™¯** | å¼€å‘æµ‹è¯• | å¤šèŠ‚ç‚¹éƒ¨ç½² | ç”Ÿäº§ç¯å¢ƒ |
| **å¹¶å‘æ•°é»˜è®¤å€¼** | 8 | 16 | 12 |
| **æœ€å¤§çˆ¬è™«æ•°** | 1 | 10 | 1 |

### å…³é”®å‘ç°

**âš ï¸ Distributed æ¨¡å¼çš„ä¸¥æ ¼è¦æ±‚**ï¼š
- ğŸš« å¿…é¡»ä½¿ç”¨ Redisï¼Œ**ä¸å…è®¸é™çº§**åˆ° Memory æ¨¡å¼
- âœ… å¯åŠ¨æ—¶ä¼šéªŒè¯ Redis è¿æ¥
- ğŸš« Redis ä¸å¯ç”¨æ—¶ä¼š**æŠ›å‡º `RuntimeError` å¹¶é€€å‡º**
- ğŸ›¡ï¸ è¿™ç¡®ä¿äº†å¤šèŠ‚ç‚¹éƒ¨ç½²æ—¶çš„ä¸€è‡´æ€§å’Œæ•°æ®å®‰å…¨æ€§

**âœ¨ Auto æ¨¡å¼çš„æ™ºèƒ½æ€§**ï¼š
- é…ç½®é˜¶æ®µä¸ä¾èµ– Redis
- è¿è¡Œæ—¶æ‰æ£€æµ‹ Redis å¯ç”¨æ€§
- æ ¹æ®æ£€æµ‹ç»“æœåŠ¨æ€é€‰æ‹©æœ€ä½³é…ç½®
- Redis ä¸å¯ç”¨æ—¶è‡ªåŠ¨é™çº§ï¼Œä¿è¯ç³»ç»Ÿå¯ç”¨æ€§

---

## Standalone æ¨¡å¼

å•æœºæ¨¡å¼ï¼Œæœ€ç®€å•ç›´æ¥ï¼Œé€‚åˆå¼€å‘æµ‹è¯•å’Œä¸­å°è§„æ¨¡çˆ¬å–ã€‚

### é…ç½®ç¤ºä¾‹

```python
from crawlo.config import CrawloConfig

config = CrawloConfig.standalone(
    project_name='my_project',
    concurrency=8,
    download_delay=1.0
)

locals().update(config.to_dict())
```

### ç”Ÿæˆçš„é…ç½®

```python
{
    'RUN_MODE': 'standalone',
    'QUEUE_TYPE': 'memory',  # å›ºå®šä¸ºå†…å­˜é˜Ÿåˆ—
    'FILTER_CLASS': 'crawlo.filters.memory_filter.MemoryFilter',
    'DEFAULT_DEDUP_PIPELINE': 'crawlo.pipelines.memory_dedup_pipeline.MemoryDedupPipeline',
    'PROJECT_NAME': 'my_project',
    'CONCURRENCY': 8,
    'MAX_RUNNING_SPIDERS': 1,
    'DOWNLOAD_DELAY': 1.0,
}
```

### è¿è¡Œæ—¶è¡Œä¸º

- âœ… **é˜Ÿåˆ—**: å§‹ç»ˆä½¿ç”¨ `MemoryQueue`
- âœ… **è¿‡æ»¤å™¨**: å§‹ç»ˆä½¿ç”¨ `MemoryFilter`
- âœ… **å»é‡ç®¡é“**: å§‹ç»ˆä½¿ç”¨ `MemoryDedupPipeline`
- âŒ **Redis æ£€æµ‹**: ä¸è¿›è¡Œæ£€æµ‹
- âŒ **é…ç½®æ›´æ–°**: ä¸ä¼šæ›´æ–°

### é€‚ç”¨åœºæ™¯

- æœ¬åœ°å¼€å‘è°ƒè¯•
- å­¦ä¹ æ¡†æ¶ç‰¹æ€§
- ä¸­å°è§„æ¨¡æ•°æ®é‡‡é›†ï¼ˆ< 10ä¸‡æ¡ï¼‰
- æ— éœ€åˆ†å¸ƒå¼éƒ¨ç½²

---

## Distributed æ¨¡å¼

åˆ†å¸ƒå¼æ¨¡å¼ï¼Œä¸¥æ ¼è¦æ±‚ Redis å¯ç”¨ï¼Œé€‚åˆå¤šèŠ‚ç‚¹ååŒå·¥ä½œã€‚

### é…ç½®ç¤ºä¾‹

```python
from crawlo.config import CrawloConfig

config = CrawloConfig.distributed(
    project_name='my_distributed_project',
    redis_host='redis.example.com',
    redis_port=6379,
    redis_password='your_password',
    redis_db=0,
    concurrency=16,
    download_delay=1.0
)

locals().update(config.to_dict())
```

### ç”Ÿæˆçš„é…ç½®

```python
{
    'RUN_MODE': 'distributed',
    'QUEUE_TYPE': 'redis',  # å¿…é¡»ä½¿ç”¨ Redis
    'FILTER_CLASS': 'crawlo.filters.aioredis_filter.AioRedisFilter',
    'DEFAULT_DEDUP_PIPELINE': 'crawlo.pipelines.redis_dedup_pipeline.RedisDedupPipeline',
    'REDIS_HOST': 'redis.example.com',
    'REDIS_PORT': 6379,
    'REDIS_PASSWORD': 'your_password',
    'REDIS_DB': 0,
    'REDIS_URL': 'redis://:your_password@redis.example.com:6379/0',
    'PROJECT_NAME': 'my_distributed_project',
    'SCHEDULER_QUEUE_NAME': 'crawlo:my_distributed_project:queue:requests',
    'CONCURRENCY': 16,
    'MAX_RUNNING_SPIDERS': 10,
    'DOWNLOAD_DELAY': 1.0,
}
```

### è¿è¡Œæ—¶è¡Œä¸º

- âœ… **Redis æ£€æµ‹**: å¯åŠ¨æ—¶å¼ºåˆ¶æ£€æŸ¥ Redis è¿æ¥
- ğŸš« **ä¸å…è®¸é™çº§**: Redis ä¸å¯ç”¨æ—¶æŠ›å‡º `RuntimeError` å¹¶é€€å‡º
- âœ… **é˜Ÿåˆ—**: å¿…é¡»ä½¿ç”¨ `RedisPriorityQueue`
- âœ… **è¿‡æ»¤å™¨**: å¿…é¡»ä½¿ç”¨ `AioRedisFilter`
- âœ… **å»é‡ç®¡é“**: å¿…é¡»ä½¿ç”¨ `RedisDedupPipeline`

### Redis ä¸å¯ç”¨æ—¶çš„é”™è¯¯ä¿¡æ¯

```bash
$ crawlo run my_spider

2025-10-25 22:00:00 - [queue_manager] - ERROR: Distributed æ¨¡å¼è¦æ±‚ Redis å¯ç”¨ï¼Œä½†æ— æ³•è¿æ¥åˆ° Redis æœåŠ¡å™¨ã€‚
é”™è¯¯ä¿¡æ¯: Connection refused
Redis URL: redis://127.0.0.1:6379/0
è¯·æ£€æŸ¥ï¼š
  1. Redis æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ
  2. Redis è¿æ¥é…ç½®æ˜¯å¦æ­£ç¡®
  3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸

RuntimeError: Distributed æ¨¡å¼è¦æ±‚ Redis å¯ç”¨ï¼Œä½†æ— æ³•è¿æ¥åˆ° Redis æœåŠ¡å™¨ã€‚
```

### ä¸ºä»€ä¹ˆè¦ä¸¥æ ¼è¦æ±‚ Redisï¼Ÿ

1. **æ•°æ®ä¸€è‡´æ€§**: é˜²æ­¢ä¸åŒèŠ‚ç‚¹ä½¿ç”¨ä¸åŒçš„é˜Ÿåˆ—ç±»å‹
2. **å»é‡æœ‰æ•ˆæ€§**: ç¡®ä¿å¤šèŠ‚ç‚¹é—´çš„å»é‡åŠŸèƒ½æ­£å¸¸å·¥ä½œ
3. **ä»»åŠ¡åˆ†é…**: é˜²æ­¢ä»»åŠ¡è¢«é‡å¤æ‰§è¡Œ
4. **é—®é¢˜æ—©å‘ç°**: å¯åŠ¨å¤±è´¥æ¯”è¿è¡Œæ—¶å¤±è´¥æ›´å®¹æ˜“å‘ç°å’Œä¿®å¤
5. **æ˜ç¡®çš„æ„å›¾**: åˆ†å¸ƒå¼æ¨¡å¼å°±åº”è¯¥æ˜¯åˆ†å¸ƒå¼çš„ï¼Œä¸åº”è¯¥é™é»˜é™çº§

### é€‚ç”¨åœºæ™¯

- å¤šæœåŠ¡å™¨ååŒé‡‡é›†
- å¤§è§„æ¨¡æ•°æ®é‡‡é›†ï¼ˆ> ç™¾ä¸‡æ¡ï¼‰
- éœ€è¦ä¸¥æ ¼ä¿è¯åˆ†å¸ƒå¼ä¸€è‡´æ€§
- ç”Ÿäº§ç¯å¢ƒå¤šèŠ‚ç‚¹éƒ¨ç½²

---

## Auto æ¨¡å¼

è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ï¼Œæ™ºèƒ½é€‰æ‹©æœ€ä½³è¿è¡Œæ–¹å¼ï¼Œæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒã€‚

### é…ç½®ç¤ºä¾‹

```python
from crawlo.config import CrawloConfig

config = CrawloConfig.auto(
    project_name='my_auto_project',
    concurrency=12,
    download_delay=1.0
)

locals().update(config.to_dict())

# å¯é€‰ï¼šé…ç½® Redisï¼ˆå¦‚æœå¯ç”¨ä¼šè‡ªåŠ¨ä½¿ç”¨ï¼‰
REDIS_HOST = '127.0.0.1'
REDIS_PORT = 6379
REDIS_URL = 'redis://127.0.0.1:6379/0'
```

### åˆå§‹é…ç½®

```python
{
    'RUN_MODE': 'auto',
    'QUEUE_TYPE': 'auto',  # è¿è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹
    'FILTER_CLASS': 'crawlo.filters.memory_filter.MemoryFilter',  # é»˜è®¤å€¼
    'DEFAULT_DEDUP_PIPELINE': 'crawlo.pipelines.memory_dedup_pipeline.MemoryDedupPipeline',
    'PROJECT_NAME': 'my_auto_project',
    'CONCURRENCY': 12,
    'MAX_RUNNING_SPIDERS': 1,
    'DOWNLOAD_DELAY': 1.0,
}
```

### è¿è¡Œæ—¶æ£€æµ‹é€»è¾‘

```
Scheduler åˆå§‹åŒ–æ—¶
  â””â”€ æ£€æŸ¥ REDIS_URL æ˜¯å¦é…ç½®
      â”‚
      â”œâ”€ [å·²é…ç½® Redis] â†’ å°è¯•è¿æ¥
      â”‚   â”‚
      â”‚   â”œâ”€ [è¿æ¥æˆåŠŸ] âœ“
      â”‚   â”‚   â”œâ”€ ä½¿ç”¨ RedisPriorityQueue
      â”‚   â”‚   â”œâ”€ æ›´æ–°ä¸º AioRedisFilter
      â”‚   â”‚   â”œâ”€ æ›´æ–°ä¸º RedisDedupPipeline
      â”‚   â”‚   â””â”€ é…ç½®è‡ªåŠ¨æ›´æ–°
      â”‚   â”‚
      â”‚   â””â”€ [è¿æ¥å¤±è´¥] âœ—
      â”‚       â”œâ”€ ä½¿ç”¨ MemoryQueue
      â”‚       â”œâ”€ ä½¿ç”¨ MemoryFilter
      â”‚       â””â”€ ä½¿ç”¨ MemoryDedupPipeline
      â”‚
      â””â”€ [æœªé…ç½® Redis]
          â”œâ”€ ä½¿ç”¨ MemoryQueue
          â”œâ”€ ä½¿ç”¨ MemoryFilter
          â””â”€ ä½¿ç”¨ MemoryDedupPipeline
```

### è¿è¡Œç¤ºä¾‹

**åœºæ™¯ 1: Redis å¯ç”¨**

```bash
$ crawlo run my_spider

2025-10-25 22:00:00 - [crawlo.framework] - INFO: Run mode: auto
2025-10-25 22:00:00 - [queue_manager] - DEBUG: Auto-detection: Redis available, using distributed queue
2025-10-25 22:00:00 - [scheduler] - INFO: enabled filters: 
  crawlo.filters.aioredis_filter.AioRedisFilter
```

**åœºæ™¯ 2: Redis ä¸å¯ç”¨**

```bash
$ crawlo run my_spider

2025-10-25 22:00:00 - [crawlo.framework] - INFO: Run mode: auto
2025-10-25 22:00:00 - [queue_manager] - DEBUG: Auto-detection: Redis not configured, using memory queue
2025-10-25 22:00:00 - [scheduler] - INFO: enabled filters: 
  crawlo.filters.memory_filter.MemoryFilter
```

### é€‚ç”¨åœºæ™¯

- ç”Ÿäº§ç¯å¢ƒå•èŠ‚ç‚¹éƒ¨ç½²
- éœ€è¦å®¹é”™æ€§çš„åœºæ™¯
- å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒå…±ç”¨é…ç½®
- ä¸ç¡®å®š Redis å¯ç”¨æ€§çš„åœºæ™¯

---

## é…ç½®æ›´æ–°æœºåˆ¶

### æ›´æ–°æ—¶æœº

é…ç½®æ›´æ–°å‘ç”Ÿåœ¨ Scheduler åˆå§‹åŒ–é˜¶æ®µï¼š

```python
# crawlo/core/scheduler.py
async def open(self, spider):
    """æ‰“å¼€è°ƒåº¦å™¨"""
    self.spider = spider
    
    # åˆå§‹åŒ–é˜Ÿåˆ—ç®¡ç†å™¨ï¼ˆå¯èƒ½è§¦å‘ Redis æ£€æµ‹ï¼‰
    needs_config_update = await self.queue_manager.initialize()
    
    # å¦‚æœæ£€æµ‹åˆ° Redis å¯ç”¨ï¼Œæ›´æ–°ç›¸å…³é…ç½®
    if needs_config_update:
        updated_configs = self._check_filter_config()
        await self._process_filter_updates(needs_config_update, updated_configs)
```

### æ›´æ–°å†…å®¹

å½“ä» Memory åˆ‡æ¢åˆ° Redis æ—¶ï¼Œä¼šè‡ªåŠ¨æ›´æ–°ï¼š

1. **FILTER_CLASS**: `MemoryFilter` â†’ `AioRedisFilter`
2. **DEFAULT_DEDUP_PIPELINE**: `MemoryDedupPipeline` â†’ `RedisDedupPipeline`
3. **PIPELINES**: è‡ªåŠ¨æ›¿æ¢å»é‡ç®¡é“

### å“ªäº›æ¨¡å¼ä¼šè§¦å‘é…ç½®æ›´æ–°ï¼Ÿ

- âœ… **Auto æ¨¡å¼**: æ£€æµ‹åˆ° Redis å¯ç”¨æ—¶ä¼šæ›´æ–°
- âŒ **Standalone æ¨¡å¼**: ä¸ä¼šæ›´æ–°
- âŒ **Distributed æ¨¡å¼**: ä¸ä¼šæ›´æ–°ï¼ˆé…ç½®å·²ç¡®å®šï¼‰

---

## æœ€ä½³å®è·µ

### å¼€å‘ç¯å¢ƒ

```python
# ä½¿ç”¨ standalone æ¨¡å¼ï¼Œç®€å•å¿«é€Ÿ
config = CrawloConfig.standalone(
    project_name='dev_project',
    concurrency=4,
    download_delay=2.0
)
```

**ä¼˜ç‚¹**ï¼š
- æ— éœ€å®‰è£… Redis
- å¯åŠ¨é€Ÿåº¦å¿«
- è°ƒè¯•æ–¹ä¾¿

### æµ‹è¯•ç¯å¢ƒ

```python
# ä½¿ç”¨ auto æ¨¡å¼ï¼Œå¯ä»¥æµ‹è¯•ä¸¤ç§åœºæ™¯
config = CrawloConfig.auto(
    project_name='test_project',
    concurrency=8,
    download_delay=1.5
)

# å¯é€‰ï¼šé…ç½® Redis ç”¨äºæµ‹è¯•åˆ†å¸ƒå¼åœºæ™¯
REDIS_URL = 'redis://127.0.0.1:6379/1'  # ä½¿ç”¨ä¸åŒçš„ DB
```

**ä¼˜ç‚¹**ï¼š
- å¯ä»¥æµ‹è¯• Redis å’Œ Memory ä¸¤ç§åœºæ™¯
- é…ç½®çµæ´»
- æ¥è¿‘ç”Ÿäº§ç¯å¢ƒ

### ç”Ÿäº§ç¯å¢ƒï¼ˆå•èŠ‚ç‚¹ï¼‰

```python
# ä½¿ç”¨ auto æ¨¡å¼ï¼Œäº«å— Redis æ€§èƒ½æå‡
# åŒæ—¶ä¿è¯ Redis æ•…éšœæ—¶èƒ½å¤Ÿé™çº§è¿è¡Œ
config = CrawloConfig.auto(
    project_name='prod_project',
    concurrency=16,
    download_delay=1.0
)

# é…ç½® Redis
REDIS_HOST = 'redis.internal.com'
REDIS_PORT = 6379
REDIS_PASSWORD = 'your_password'
REDIS_DB = 0
```

**ä¼˜ç‚¹**ï¼š
- è‡ªåŠ¨ä½¿ç”¨ Redis æå‡æ€§èƒ½
- Redis æ•…éšœæ—¶è‡ªåŠ¨é™çº§
- å®¹é”™æ€§å¥½

### ç”Ÿäº§ç¯å¢ƒï¼ˆå¤šèŠ‚ç‚¹ï¼‰

```python
# ä½¿ç”¨ distributed æ¨¡å¼ï¼Œæ˜ç¡®å£°æ˜åˆ†å¸ƒå¼éƒ¨ç½²
config = CrawloConfig.distributed(
    project_name='prod_distributed',
    redis_host='redis-cluster.internal.com',
    redis_port=6379,
    redis_password='your_password',
    redis_db=0,
    concurrency=32,
    download_delay=0.5
)
```

**ä¼˜ç‚¹**ï¼š
- ä¸¥æ ¼ä¿è¯åˆ†å¸ƒå¼ä¸€è‡´æ€§
- å¤šèŠ‚ç‚¹ååŒå·¥ä½œ
- é—®é¢˜æ—©å‘ç°
- æ•°æ®å®‰å…¨æ€§é«˜

---

## å¸¸è§é—®é¢˜

### Q1: Auto æ¨¡å¼ä¸‹ï¼Œå¦‚ä½•ç¡®ä¿ä¸€å®šä½¿ç”¨ Redisï¼Ÿ

**A**: Auto æ¨¡å¼ä¼šä¼˜å…ˆä½¿ç”¨ Redisï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œä½†å¦‚æœéœ€è¦ç¡®ä¿ä½¿ç”¨ Redisï¼Œå»ºè®®ï¼š

1. ä½¿ç”¨ `distributed` æ¨¡å¼ï¼ˆRedis ä¸å¯ç”¨æ—¶ä¼šæŠ¥é”™ï¼‰
2. åœ¨å¯åŠ¨å‰æ£€æŸ¥ Redis è¿æ¥ï¼ˆ`crawlo run` ä¼šè‡ªåŠ¨æ£€æŸ¥ï¼‰
3. ç›‘æ§æ—¥å¿—ä¸­çš„ "Queue type: redis" ä¿¡æ¯

### Q2: Distributed æ¨¡å¼ä¸‹ Redis ä¸å¯ç”¨ä¼šæ€æ ·ï¼Ÿ

**A**: Distributed æ¨¡å¼ä¸‹ Redis å¿…é¡»å¯ç”¨ï¼š
- ğŸš« ä¸ä¼šé™çº§åˆ° Memory æ¨¡å¼
- ğŸš« ä¼šæŠ›å‡º `RuntimeError` å¹¶é€€å‡º
- âœ… é”™è¯¯ä¿¡æ¯ä¼šæç¤ºå…·ä½“çš„æ£€æŸ¥æ­¥éª¤
- ğŸ¯ è¿™ç¡®ä¿äº†å¤šèŠ‚ç‚¹éƒ¨ç½²æ—¶çš„ä¸€è‡´æ€§

### Q3: é…ç½®æ›´æ–°ä¼šå½±å“å“ªäº›ç»„ä»¶ï¼Ÿ

**A**: é…ç½®æ›´æ–°ä¸»è¦å½±å“ï¼š
1. **è¿‡æ»¤å™¨ï¼ˆFilterï¼‰**: MemoryFilter â†” AioRedisFilter
2. **å»é‡ç®¡é“ï¼ˆDedup Pipelineï¼‰**: MemoryDedupPipeline â†” RedisDedupPipeline
3. **å…¶ä»–ä½¿ç”¨è¿™äº›é…ç½®çš„ç»„ä»¶**

é˜Ÿåˆ—ï¼ˆQueueï¼‰æœ¬èº«åœ¨åˆå§‹åŒ–æ—¶å°±å·²ç»ç¡®å®šï¼Œä¸ä¼šåœ¨è¿è¡Œä¸­åˆ‡æ¢ã€‚

### Q4: å¦‚ä½•åœ¨ä»£ç ä¸­åˆ¤æ–­å½“å‰ä½¿ç”¨çš„æ˜¯å“ªç§æ¨¡å¼ï¼Ÿ

**A**: å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åˆ¤æ–­ï¼š

```python
# åœ¨çˆ¬è™«ä¸­
run_mode = self.crawler.settings.get('RUN_MODE')
queue_type = self.crawler.settings.get('QUEUE_TYPE')

# é€šè¿‡é˜Ÿåˆ—ç®¡ç†å™¨è·å–å®é™…ä½¿ç”¨çš„ç±»å‹
status = self.crawler.engine.scheduler.queue_manager.get_status()
actual_queue_type = status['type']  # 'memory' æˆ– 'redis'
print(f"å®é™…é˜Ÿåˆ—ç±»å‹: {actual_queue_type}")
print(f"å¥åº·çŠ¶æ€: {status['health']}")
```

### Q5: ä¸ºä»€ä¹ˆ Distributed æ¨¡å¼è¦ä¸¥æ ¼è¦æ±‚ Redis å¯ç”¨ï¼Ÿ

**A**: ä¸¥æ ¼è¦æ±‚çš„åŸå› ï¼š
1. **æ•°æ®ä¸€è‡´æ€§**: é˜²æ­¢ä¸åŒèŠ‚ç‚¹ä½¿ç”¨ä¸åŒçš„é˜Ÿåˆ—ç±»å‹
2. **å»é‡æœ‰æ•ˆæ€§**: ç¡®ä¿å¤šèŠ‚ç‚¹é—´çš„å»é‡åŠŸèƒ½æ­£å¸¸å·¥ä½œ
3. **ä»»åŠ¡åˆ†é…**: é˜²æ­¢ä»»åŠ¡è¢«é‡å¤æ‰§è¡Œ
4. **é—®é¢˜æ—©å‘ç°**: å¯åŠ¨å¤±è´¥æ¯”è¿è¡Œæ—¶å¤±è´¥æ›´å®¹æ˜“å‘ç°å’Œä¿®å¤
5. **æ˜ç¡®çš„æ„å›¾**: åˆ†å¸ƒå¼æ¨¡å¼å°±åº”è¯¥æ˜¯åˆ†å¸ƒå¼çš„ï¼Œä¸åº”è¯¥é™é»˜é™çº§

å¦‚æœéœ€è¦å®¹é”™æ€§ï¼Œåº”è¯¥ä½¿ç”¨ `auto` æ¨¡å¼è€Œé `distributed` æ¨¡å¼ã€‚

---

## å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# æŸ¥çœ‹çˆ¬è™«åˆ—è¡¨ï¼ˆä¼šæ˜¾ç¤ºè¿è¡Œæ¨¡å¼ï¼‰
crawlo list

# è¿è¡Œçˆ¬è™«ï¼ˆä¼šè‡ªåŠ¨æ£€æµ‹ Redisï¼‰
crawlo run my_spider

# æŸ¥çœ‹å®é™…ä½¿ç”¨çš„é˜Ÿåˆ—ç±»å‹ï¼ˆè§‚å¯Ÿæ—¥å¿—ï¼‰
crawlo run my_spider | grep "Queue type"
```

---

## æ¨èä½¿ç”¨åœºæ™¯æ€»ç»“

| åœºæ™¯ | æ¨èæ¨¡å¼ | ç†ç”± |
|------|---------|------|
| æœ¬åœ°å¼€å‘ | `standalone` | ç®€å•å¿«é€Ÿï¼Œæ— éœ€ Redis |
| CI/CD æµ‹è¯• | `auto` | å¯æµ‹è¯•ä¸¤ç§åœºæ™¯ |
| ç”Ÿäº§å•èŠ‚ç‚¹ | `auto` | æ™ºèƒ½é€‰æ‹©ï¼Œå®¹é”™æ€§å¥½ |
| ç”Ÿäº§å¤šèŠ‚ç‚¹ | `distributed` | ä¸¥æ ¼ä¿è¯ä¸€è‡´æ€§ |

---

## ç›¸å…³æ–‡æ¡£

- [åˆ†å¸ƒå¼çˆ¬è™«æ•™ç¨‹](./distributed_crawling.md)
- [ç¬¬ä¸€ä¸ªçˆ¬è™«](./first_spider.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æ›´æ–°æ—¶é—´**: 2025-10-25  
**é€‚ç”¨ç‰ˆæœ¬**: Crawlo 1.4.7+
