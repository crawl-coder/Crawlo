# Crawloæ¡†æ¶é‡æ„è¿ç§»æŒ‡å—

## ğŸ¯ é‡æ„æ¦‚è¿°

Crawloæ¡†æ¶ç»è¿‡é‡å¤§é‡æ„ï¼Œä¸»è¦æ”¹è¿›åŒ…æ‹¬ï¼š

1. **ç»Ÿä¸€åˆå§‹åŒ–ç³»ç»Ÿ** - è§£å†³åˆå§‹åŒ–æ··ä¹±å’Œæ­»é”é—®é¢˜
2. **ç®€åŒ–æ—¥å¿—ç³»ç»Ÿ** - ç§»é™¤è¿‡åº¦è®¾è®¡ï¼Œæå‡æ€§èƒ½
3. **æ¨¡å—åŒ–æ¶æ„** - æ¸…æ™°çš„ç»„ä»¶è¾¹ç•Œå’Œä¾èµ–å…³ç³»
4. **ç»„ä»¶å·¥å‚** - æ”¯æŒä¾èµ–æ³¨å…¥å’Œæµ‹è¯•

## ğŸ”„ APIå˜æ›´

### 1. æ—¥å¿—ç³»ç»Ÿè¿ç§»

**æ—§å†™æ³•ï¼š**
```python
from crawlo.utils.log import get_logger, LoggerManager

# å¤æ‚çš„å»¶è¿Ÿåˆå§‹åŒ–
logger = None
def get_module_logger():
    global logger
    if logger is None:
        logger = get_logger(__name__)
    return logger

# å¤æ‚çš„é…ç½®
LoggerManager.configure(settings)
```

**æ–°å†™æ³•ï¼š**
```python
from crawlo.logging import get_logger, configure_logging

# ç®€å•ç›´æ¥
logger = get_logger(__name__)

# ç®€åŒ–çš„é…ç½®
configure_logging(LOG_LEVEL='INFO', LOG_FILE='logs/app.log')
```

### 2. æ¡†æ¶åˆå§‹åŒ–è¿ç§»

**æ—§å†™æ³•ï¼š**
```python
from crawlo.crawler import CrawlerProcess
from crawlo.core.framework_initializer import get_framework_initializer

# å¤æ‚çš„åˆå§‹åŒ–æ£€æŸ¥
init_manager = get_framework_initializer()
if not init_manager.is_ready:
    init_manager.ensure_framework_initialized()

process = CrawlerProcess(settings)
```

**æ–°å†™æ³•ï¼š**
```python
from crawlo.framework import get_framework

# è‡ªåŠ¨åˆå§‹åŒ–
framework = get_framework(settings)
```

### 3. çˆ¬è™«è¿è¡Œè¿ç§»

**æ—§å†™æ³•ï¼š**
```python
from crawlo.crawler import CrawlerProcess
import asyncio

async def run_spider():
    process = CrawlerProcess()
    await process.crawl(MySpider)

asyncio.run(run_spider())
```

**æ–°å†™æ³•ï¼š**
```python
from crawlo.framework import run_spider
import asyncio

# æ–¹å¼1ï¼šä½¿ç”¨ä¾¿æ·å‡½æ•°
async def main():
    await run_spider(MySpider)

asyncio.run(main())

# æ–¹å¼2ï¼šä½¿ç”¨æ¡†æ¶å®ä¾‹
from crawlo.framework import get_framework

async def main():
    framework = get_framework()
    await framework.run(MySpider)

asyncio.run(main())
```

## ğŸ—ï¸ æ–°æ¶æ„ç‰¹æ€§

### 1. ç»„ä»¶å·¥å‚ç³»ç»Ÿ

```python
from crawlo.factories import get_component_registry, ComponentSpec

# æ³¨å†Œè‡ªå®šä¹‰ç»„ä»¶
def create_my_component(crawler, **kwargs):
    return MyComponent(crawler)

registry = get_component_registry()
registry.register(ComponentSpec(
    name='my_component',
    component_type=MyComponent,
    factory_func=create_my_component
))

# ä½¿ç”¨ç»„ä»¶
component = registry.create('my_component', crawler=crawler)
```

### 2. ç°ä»£åŒ–Crawler

```python
from crawlo.new_crawler import ModernCrawler
import asyncio

async def main():
    crawler = ModernCrawler(MySpider, settings)
    await crawler.crawl()
    
    # è·å–æŒ‡æ ‡
    metrics = crawler.metrics
    print(f"Success rate: {metrics.get_success_rate()}%")

asyncio.run(main())
```

### 3. ç»Ÿä¸€æ¡†æ¶å…¥å£

```python
from crawlo.framework import CrawloFramework

# åˆ›å»ºæ¡†æ¶å®ä¾‹
framework = CrawloFramework({
    'LOG_LEVEL': 'DEBUG',
    'CONCURRENCY': 16,
    'LOG_FILE': 'logs/crawler.log'
})

# è¿è¡Œçˆ¬è™«
await framework.run(MySpider)

# è¿è¡Œå¤šä¸ªçˆ¬è™«
await framework.run_multiple([Spider1, Spider2, Spider3])
```

## ğŸ“¦ å‘åå…¼å®¹æ€§

### ä¿æŒå…¼å®¹çš„API

ä»¥ä¸‹APIä¿æŒå‘åå…¼å®¹ï¼š

```python
# è¿™äº›ä»ç„¶å¯ç”¨
from crawlo.utils.log import get_logger  # è‡ªåŠ¨é‡å®šå‘åˆ°æ–°ç³»ç»Ÿ
from crawlo.crawler import Crawler, CrawlerProcess  # ä¿æŒåŸæœ‰æ¥å£
```

### é€æ­¥è¿ç§»ç­–ç•¥

1. **é˜¶æ®µ1ï¼šæ›´æ–°æ—¥å¿—ç³»ç»Ÿ**
   ```python
   # æ›¿æ¢å¤æ‚çš„å»¶è¿Ÿåˆå§‹åŒ–
   from crawlo.logging import get_logger
   logger = get_logger(__name__)
   ```

2. **é˜¶æ®µ2ï¼šä½¿ç”¨æ–°çš„æ¡†æ¶å…¥å£**
   ```python
   # æ›¿æ¢å¤æ‚çš„åˆå§‹åŒ–é€»è¾‘
   from crawlo.framework import get_framework
   framework = get_framework(settings)
   ```

3. **é˜¶æ®µ3ï¼šè¿ç§»åˆ°ç°ä»£Crawler**
   ```python
   # ä½¿ç”¨æ–°çš„Crawlerå®ç°
   from crawlo.new_crawler import ModernCrawler
   crawler = ModernCrawler(spider_cls, settings)
   ```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. ç®€å•çš„é¡¹ç›®ç»“æ„

```python
# main.py
import asyncio
from crawlo.framework import run_spider
from spiders.my_spider import MySpider

async def main():
    await run_spider(MySpider, {
        'LOG_LEVEL': 'INFO',
        'CONCURRENCY': 8
    })

if __name__ == '__main__':
    asyncio.run(main())
```

### 2. é…ç½®ç®¡ç†

```python
# config.py
CRAWLO_SETTINGS = {
    'LOG_LEVEL': 'INFO',
    'LOG_FILE': 'logs/crawler.log',
    'CONCURRENCY': 16,
    'DOWNLOAD_DELAY': 1.0
}

# main.py
from config import CRAWLO_SETTINGS
from crawlo.framework import get_framework

framework = get_framework(CRAWLO_SETTINGS)
```

### 3. æµ‹è¯•

```python
import pytest
from crawlo.framework import reset_framework, create_crawler
from crawlo.factories import get_component_registry

@pytest.fixture
def clean_framework():
    reset_framework()
    get_component_registry().clear()
    yield
    reset_framework()

def test_spider(clean_framework):
    crawler = create_crawler(TestSpider, {'LOG_LEVEL': 'DEBUG'})
    # æµ‹è¯•é€»è¾‘...
```

## âš¡ æ€§èƒ½æ”¹è¿›

### 1. æ—¥å¿—ç³»ç»Ÿä¼˜åŒ–

- ç§»é™¤å¤æ‚çš„é”ç«äº‰
- ä½¿ç”¨å¼±å¼•ç”¨é¿å…å†…å­˜æ³„æ¼
- LRUç¼“å­˜å‡å°‘é‡å¤è®¡ç®—

### 2. åˆå§‹åŒ–ä¼˜åŒ–

- æ¸…æ™°çš„é˜¶æ®µåŒ–åˆå§‹åŒ–
- é¿å…å¾ªç¯ä¾èµ–
- æ›´å¿«çš„å¯åŠ¨æ—¶é—´

### 3. å†…å­˜ä¼˜åŒ–

- ç»„ä»¶å·¥å‚å‡å°‘é‡å¤åˆ›å»º
- æ›´å¥½çš„èµ„æºç®¡ç†
- é™çº§ç­–ç•¥é¿å…å´©æºƒ

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å¯¼å…¥é”™è¯¯**
   ```python
   # ç¡®ä¿æŒ‰é¡ºåºå¯¼å…¥
   from crawlo.logging import get_logger  # å…ˆå¯¼å…¥æ—¥å¿—
   from crawlo.framework import get_framework  # å†å¯¼å…¥æ¡†æ¶
   ```

2. **é…ç½®é—®é¢˜**
   ```python
   # ä½¿ç”¨æ–°çš„é…ç½®æ–¹å¼
   from crawlo.logging import configure_logging
   configure_logging(LOG_LEVEL='DEBUG')
   ```

3. **åˆå§‹åŒ–å¤±è´¥**
   ```python
   # æ£€æŸ¥æ¡†æ¶çŠ¶æ€
   from crawlo.initialization import is_framework_ready
   if not is_framework_ready():
       # æ‰‹åŠ¨åˆå§‹åŒ–
       from crawlo.initialization import initialize_framework
       initialize_framework()
   ```

## ğŸ“š æ›´å¤šèµ„æº

- [æ–°æ¶æ„è®¾è®¡æ–‡æ¡£](./architecture.md)
- [ç»„ä»¶å·¥å‚æŒ‡å—](./component_factory.md)
- [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](./performance.md)
- [APIå‚è€ƒæ–‡æ¡£](./api_reference.md)