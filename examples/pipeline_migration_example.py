# -*- coding: utf-8 -*-
"""
Pipelineèµ„æºç®¡ç†æ”¹é€ ç¤ºä¾‹
======================

å±•ç¤ºå¦‚ä½•å°†ç°æœ‰Pipelineæ”¹é€ ä¸ºä½¿ç”¨ResourceManagerçš„ç‰ˆæœ¬

ç¤ºä¾‹åŒ…æ‹¬ï¼š
1. MongoDB Pipelineæ”¹é€ ï¼ˆæ•°æ®åº“ç±»ï¼‰
2. CSV Pipelineæ”¹é€ ï¼ˆæ–‡ä»¶ç±»ï¼‰
3. Rediså»é‡Pipelineæ”¹é€ ï¼ˆç¼“å­˜ç±»ï¼‰
"""

# ================== ç¤ºä¾‹1: MongoDB Pipelineæ”¹é€  ==================

# âŒ åŸå§‹ç‰ˆæœ¬ - å­˜åœ¨èµ„æºæ³„éœ²é£é™©
class OldMongoPipeline:
    """åŸå§‹çš„MongoDB Pipeline"""
    
    def __init__(self, crawler):
        from motor.motor_asyncio import AsyncIOMotorClient
        
        self.crawler = crawler
        self.settings = crawler.settings
        
        # ç›´æ¥åˆ›å»ºå®¢æˆ·ç«¯ï¼Œæœªæ³¨å†Œèµ„æºç®¡ç†
        self.client = AsyncIOMotorClient(
            self.settings.get('MONGO_URI', 'mongodb://localhost:27017')
        )
        self.db = self.client[self.settings.get('MONGO_DATABASE', 'crawlo_db')]
        self.collection = self.db[self.settings.get('MONGO_COLLECTION', 'items')]
        
        # âš ï¸ é—®é¢˜ï¼šæ³¨å†Œäº‹ä»¶ï¼Œä½†æ¸…ç†é€»è¾‘ä¸å®Œå–„
        crawler.subscriber.subscribe(self.spider_closed, event='spider_closed')
    
    async def process_item(self, item, spider):
        await self.collection.insert_one(dict(item))
        return item
    
    async def spider_closed(self):
        # âš ï¸ é—®é¢˜ï¼šåªæ˜¯closeï¼Œæ²¡æœ‰waitï¼Œå¯èƒ½æ³„éœ²
        if self.client:
            self.client.close()
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)


# âœ… æ”¹è¿›ç‰ˆæœ¬ - ä½¿ç”¨ResourceManager
class ImprovedMongoPipeline:
    """æ”¹è¿›çš„MongoDB Pipeline - å®Œæ•´èµ„æºç®¡ç†"""
    
    def __init__(self, crawler):
        from motor.motor_asyncio import AsyncIOMotorClient
        from crawlo.utils.resource_manager import ResourceManager, ResourceType
        from crawlo.utils.log import get_logger
        
        self.crawler = crawler
        self.settings = crawler.settings
        self.logger = get_logger(self.__class__.__name__, self.settings.get('LOG_LEVEL'))
        
        # âœ… åˆ›å»ºèµ„æºç®¡ç†å™¨
        self._resource_manager = ResourceManager(name="mongo_pipeline")
        
        # MongoDBé…ç½®
        self.mongo_uri = self.settings.get('MONGO_URI', 'mongodb://localhost:27017')
        self.db_name = self.settings.get('MONGO_DATABASE', 'crawlo_db')
        self.collection_name = self.settings.get('MONGO_COLLECTION', 'items')
        
        # å»¶è¿Ÿåˆå§‹åŒ–
        self.client = None
        self.db = None
        self.collection = None
        self._initialized = False
        
        # æ³¨å†Œå…³é—­äº‹ä»¶
        crawler.subscriber.subscribe(self._on_spider_closed, event='spider_closed')
    
    async def _ensure_initialized(self):
        """ç¡®ä¿èµ„æºå·²åˆå§‹åŒ–"""
        if self._initialized:
            return
        
        from motor.motor_asyncio import AsyncIOMotorClient
        
        # åˆ›å»ºMongoDBå®¢æˆ·ç«¯
        self.client = AsyncIOMotorClient(
            self.mongo_uri,
            maxPoolSize=self.settings.get_int('MONGO_MAX_POOL_SIZE', 100),
            minPoolSize=self.settings.get_int('MONGO_MIN_POOL_SIZE', 10)
        )
        
        # âœ… æ³¨å†Œåˆ°èµ„æºç®¡ç†å™¨
        self._resource_manager.register(
            resource=self.client,
            cleanup_func=self._close_client,
            resource_type=ResourceType.DATABASE,
            name="mongo_client"
        )
        
        self.db = self.client[self.db_name]
        self.collection = self.db[self.collection_name]
        self._initialized = True
        
        self.logger.info(f"MongoDBå®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {self.collection_name}")
    
    async def _close_client(self, client):
        """å…³é—­MongoDBå®¢æˆ·ç«¯"""
        if client:
            try:
                client.close()
                # âœ… å¯ä»¥æ·»åŠ ç­‰å¾…é€»è¾‘ç¡®ä¿å®Œå…¨å…³é—­
                import asyncio
                await asyncio.sleep(0.1)
                self.logger.info("MongoDBå®¢æˆ·ç«¯å·²å…³é—­")
            except Exception as e:
                self.logger.error(f"å…³é—­MongoDBå®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    async def process_item(self, item, spider):
        await self._ensure_initialized()
        
        try:
            result = await self.collection.insert_one(dict(item))
            self.crawler.stats.inc_value('mongodb/insert_success')
            return item
        except Exception as e:
            self.logger.error(f"MongoDBæ’å…¥å¤±è´¥: {e}")
            self.crawler.stats.inc_value('mongodb/insert_failed')
            raise
    
    async def _on_spider_closed(self):
        """çˆ¬è™«å…³é—­æ—¶æ¸…ç†èµ„æº"""
        self.logger.info("å¼€å§‹æ¸…ç†MongoDB Pipelineèµ„æº...")
        
        # âœ… ä½¿ç”¨ResourceManagerç»Ÿä¸€æ¸…ç†
        cleanup_result = await self._resource_manager.cleanup_all()
        
        if cleanup_result['success_count'] > 0:
            self.logger.info(
                f"MongoDBèµ„æºæ¸…ç†å®Œæˆ: æˆåŠŸ {cleanup_result['success_count']} ä¸ª"
            )
        
        if cleanup_result['errors']:
            self.logger.warning(f"æ¸…ç†æ—¶å‡ºç°é”™è¯¯: {cleanup_result['errors']}")
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)


# ================== ç¤ºä¾‹2: CSV Pipelineæ”¹é€  ==================

# âŒ åŸå§‹ç‰ˆæœ¬ - å­˜åœ¨æ–‡ä»¶å¥æŸ„æ³„éœ²é£é™©
class OldCsvPipeline:
    """åŸå§‹çš„CSV Pipeline"""
    
    def __init__(self, crawler):
        import csv
        from pathlib import Path
        
        self.crawler = crawler
        self.settings = crawler.settings
        
        # æ–‡ä»¶è·¯å¾„
        self.file_path = Path(f"output/{crawler.spider.name}.csv")
        self.file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # âš ï¸ é—®é¢˜ï¼šç›´æ¥æ‰“å¼€æ–‡ä»¶ï¼Œæœªæ³¨å†Œèµ„æºç®¡ç†
        self.file_handle = open(self.file_path, 'w', newline='', encoding='utf-8')
        self.csv_writer = csv.writer(self.file_handle)
        
        # æ‰¹é‡ç¼“å†²åŒº
        self.batch_buffer = []
        self.batch_size = 100
        
        crawler.subscriber.subscribe(self.spider_closed, event='spider_closed')
    
    async def process_item(self, item, spider):
        self.batch_buffer.append(list(dict(item).values()))
        
        if len(self.batch_buffer) >= self.batch_size:
            await self._flush_batch()
        
        return item
    
    async def _flush_batch(self):
        # âš ï¸ é—®é¢˜ï¼šç¼ºå°‘å¼‚å¸¸å¤„ç†
        for row in self.batch_buffer:
            self.csv_writer.writerow(row)
        self.file_handle.flush()
        self.batch_buffer.clear()
    
    async def spider_closed(self):
        # åˆ·æ–°å‰©ä½™æ•°æ®
        await self._flush_batch()
        
        # âš ï¸ é—®é¢˜ï¼šç¼ºå°‘å¼‚å¸¸å¤„ç†å’Œclosedæ£€æŸ¥
        if self.file_handle:
            self.file_handle.close()
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)


# âœ… æ”¹è¿›ç‰ˆæœ¬ - ä½¿ç”¨ResourceManager
class ImprovedCsvPipeline:
    """æ”¹è¿›çš„CSV Pipeline - å®Œæ•´èµ„æºç®¡ç†"""
    
    def __init__(self, crawler):
        import asyncio
        from pathlib import Path
        from datetime import datetime
        from crawlo.utils.resource_manager import ResourceManager, ResourceType
        from crawlo.utils.log import get_logger
        
        self.crawler = crawler
        self.settings = crawler.settings
        self.logger = get_logger(self.__class__.__name__, self.settings.get('LOG_LEVEL'))
        
        # âœ… åˆ›å»ºèµ„æºç®¡ç†å™¨
        self._resource_manager = ResourceManager(name="csv_pipeline")
        
        # æ–‡ä»¶è·¯å¾„
        spider_name = crawler.spider.name
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.file_path = Path(f"output/{spider_name}_{timestamp}.csv")
        self.file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å»¶è¿Ÿåˆå§‹åŒ–
        self.file_handle = None
        self.csv_writer = None
        self._file_lock = asyncio.Lock()
        self._initialized = False
        
        # æ‰¹é‡é…ç½®
        self.batch_buffer = []
        self.batch_size = self.settings.get_int('CSV_BATCH_SIZE', 100)
        
        # æ³¨å†Œå…³é—­äº‹ä»¶
        crawler.subscriber.subscribe(self._on_spider_closed, event='spider_closed')
    
    async def _ensure_initialized(self):
        """ç¡®ä¿æ–‡ä»¶å·²æ‰“å¼€"""
        if self._initialized:
            return
        
        async with self._file_lock:
            if not self._initialized:
                import csv
                
                # æ‰“å¼€æ–‡ä»¶
                self.file_handle = open(
                    self.file_path, 
                    'w', 
                    newline='', 
                    encoding='utf-8'
                )
                
                # âœ… æ³¨å†Œåˆ°èµ„æºç®¡ç†å™¨
                self._resource_manager.register(
                    resource=self.file_handle,
                    cleanup_func=self._close_file,
                    resource_type=ResourceType.OTHER,
                    name=str(self.file_path)
                )
                
                self.csv_writer = csv.writer(self.file_handle)
                self._initialized = True
                
                self.logger.info(f"CSVæ–‡ä»¶å·²æ‰“å¼€: {self.file_path}")
    
    async def _close_file(self, file_handle):
        """å…³é—­æ–‡ä»¶å¥æŸ„"""
        if file_handle and not file_handle.closed:
            try:
                # âœ… å…ˆåˆ·æ–°ç¼“å†²åŒº
                file_handle.flush()
                file_handle.close()
                self.logger.info(f"CSVæ–‡ä»¶å·²å…³é—­: {self.file_path}")
            except Exception as e:
                self.logger.error(f"å…³é—­æ–‡ä»¶å¤±è´¥: {e}")
    
    async def process_item(self, item, spider):
        await self._ensure_initialized()
        
        item_dict = dict(item)
        row = list(item_dict.values())
        
        async with self._file_lock:
            self.batch_buffer.append(row)
            
            if len(self.batch_buffer) >= self.batch_size:
                await self._flush_batch()
        
        return item
    
    async def _flush_batch(self):
        """åˆ·æ–°æ‰¹é‡ç¼“å†²åŒº"""
        if not self.batch_buffer:
            return
        
        try:
            # âœ… æ·»åŠ å¼‚å¸¸å¤„ç†
            for row in self.batch_buffer:
                self.csv_writer.writerow(row)
            
            self.file_handle.flush()
            
            count = len(self.batch_buffer)
            self.batch_buffer.clear()
            
            self.logger.debug(f"æ‰¹é‡å†™å…¥ {count} è¡Œåˆ°CSVæ–‡ä»¶")
            self.crawler.stats.inc_value('csv/batch_written', count=count)
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
            # âœ… ä¸æ¸…ç©ºç¼“å†²åŒºï¼Œä¿ç•™æ•°æ®
            raise
    
    async def _on_spider_closed(self):
        """çˆ¬è™«å…³é—­æ—¶æ¸…ç†èµ„æº"""
        self.logger.info("å¼€å§‹æ¸…ç†CSV Pipelineèµ„æº...")
        
        try:
            # âœ… å…ˆåˆ·æ–°å‰©ä½™æ•°æ®
            if self.batch_buffer:
                await self._flush_batch()
                self.logger.info(f"åˆ·æ–°å‰©ä½™æ•°æ®: {len(self.batch_buffer)} è¡Œ")
        except Exception as e:
            self.logger.error(f"åˆ·æ–°æ‰¹é‡æ•°æ®å¤±è´¥: {e}")
        
        # âœ… ä½¿ç”¨ResourceManagerç»Ÿä¸€æ¸…ç†
        cleanup_result = await self._resource_manager.cleanup_all()
        
        if cleanup_result['success_count'] > 0:
            self.logger.info(
                f"CSVèµ„æºæ¸…ç†å®Œæˆ: æˆåŠŸ {cleanup_result['success_count']} ä¸ª"
            )
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)


# ================== ç¤ºä¾‹3: Rediså»é‡Pipelineæ”¹é€  ==================

# âŒ åŸå§‹ç‰ˆæœ¬ - å­˜åœ¨Redisè¿æ¥æ³„éœ²
class OldRedisDedupPipeline:
    """åŸå§‹çš„Rediså»é‡Pipeline"""
    
    def __init__(self, crawler):
        import redis
        
        self.crawler = crawler
        self.settings = crawler.settings
        
        # âš ï¸ é—®é¢˜ï¼šç›´æ¥åˆ›å»ºè¿æ¥ï¼Œæœªæ³¨å†Œèµ„æºç®¡ç†
        self.redis_client = redis.Redis(
            host=self.settings.get('REDIS_HOST', 'localhost'),
            port=self.settings.get_int('REDIS_PORT', 6379),
            decode_responses=True
        )
        
        self.redis_key = 'crawlo:item:fingerprint'
        self.dropped_count = 0
    
    def process_item(self, item, spider):
        from crawlo.utils.fingerprint import FingerprintGenerator
        
        fingerprint = FingerprintGenerator.item_fingerprint(item)
        is_new = self.redis_client.sadd(self.redis_key, fingerprint)
        
        if not is_new:
            self.dropped_count += 1
            from crawlo.exceptions import ItemDiscard
            raise ItemDiscard(f"é‡å¤item: {fingerprint}")
        
        return item
    
    def close_spider(self, spider):
        # âš ï¸ é—®é¢˜ï¼šæœªå…³é—­Redisè¿æ¥ï¼
        print(f"Dropped {self.dropped_count} items")
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)


# âœ… æ”¹è¿›ç‰ˆæœ¬ - ä½¿ç”¨ResourceManager
class ImprovedRedisDedupPipeline:
    """æ”¹è¿›çš„Rediså»é‡Pipeline - å®Œæ•´èµ„æºç®¡ç†"""
    
    def __init__(self, crawler):
        import redis
        from crawlo.utils.resource_manager import ResourceManager, ResourceType
        from crawlo.utils.log import get_logger
        
        self.crawler = crawler
        self.settings = crawler.settings
        self.logger = get_logger(self.__class__.__name__, self.settings.get('LOG_LEVEL'))
        
        # âœ… åˆ›å»ºèµ„æºç®¡ç†å™¨
        self._resource_manager = ResourceManager(name="redis_dedup_pipeline")
        
        # Redisé…ç½®
        self.redis_host = self.settings.get('REDIS_HOST', 'localhost')
        self.redis_port = self.settings.get_int('REDIS_PORT', 6379)
        self.redis_db = self.settings.get_int('REDIS_DB', 0)
        self.redis_password = self.settings.get('REDIS_PASSWORD') or None
        
        # Redisé”®å
        project_name = self.settings.get('PROJECT_NAME', 'default')
        self.redis_key = f"crawlo:{project_name}:item:fingerprint"
        
        # å»¶è¿Ÿåˆå§‹åŒ–
        self.redis_client = None
        self._initialized = False
        
        self.dropped_count = 0
        
        # æ³¨å†Œå…³é—­äº‹ä»¶
        crawler.subscriber.subscribe(self._on_spider_closed, event='spider_closed')
    
    def _ensure_initialized(self):
        """ç¡®ä¿Rediså®¢æˆ·ç«¯å·²åˆå§‹åŒ–"""
        if self._initialized:
            return
        
        import redis
        
        # åˆ›å»ºRediså®¢æˆ·ç«¯
        self.redis_client = redis.Redis(
            host=self.redis_host,
            port=self.redis_port,
            db=self.redis_db,
            password=self.redis_password,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=5
        )
        
        # æµ‹è¯•è¿æ¥
        self.redis_client.ping()
        
        # âœ… æ³¨å†Œåˆ°èµ„æºç®¡ç†å™¨
        self._resource_manager.register(
            resource=self.redis_client,
            cleanup_func=self._close_client,
            resource_type=ResourceType.NETWORK,
            name="redis_client"
        )
        
        self._initialized = True
        self.logger.info(f"Rediså®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {self.redis_host}:{self.redis_port}")
    
    def _close_client(self, client):
        """å…³é—­Rediså®¢æˆ·ç«¯"""
        if client:
            try:
                # âœ… æ˜¾å¼å…³é—­è¿æ¥
                client.close()
                self.logger.info("Rediså®¢æˆ·ç«¯å·²å…³é—­")
            except Exception as e:
                self.logger.error(f"å…³é—­Rediså®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    def process_item(self, item, spider):
        self._ensure_initialized()
        
        from crawlo.utils.fingerprint import FingerprintGenerator
        from crawlo.exceptions import ItemDiscard
        
        try:
            fingerprint = FingerprintGenerator.item_fingerprint(item)
            is_new = self.redis_client.sadd(self.redis_key, fingerprint)
            
            if not is_new:
                self.dropped_count += 1
                self.logger.debug(f"ä¸¢å¼ƒé‡å¤item: {fingerprint[:20]}...")
                raise ItemDiscard(f"é‡å¤item: {fingerprint}")
            
            return item
            
        except ItemDiscard:
            raise
        except Exception as e:
            # âœ… Redisé”™è¯¯æ—¶ç»§ç»­å¤„ç†ï¼Œé¿å…ä¸¢å¤±æ•°æ®
            self.logger.error(f"Redisé”™è¯¯: {e}")
            return item
    
    async def _on_spider_closed(self):
        """çˆ¬è™«å…³é—­æ—¶æ¸…ç†èµ„æº"""
        self.logger.info("å¼€å§‹æ¸…ç†Rediså»é‡Pipelineèµ„æº...")
        
        # è®°å½•ç»Ÿè®¡
        if self.redis_client:
            try:
                total_items = self.redis_client.scard(self.redis_key)
                self.logger.info(f"å»é‡ç»Ÿè®¡: ä¸¢å¼ƒ {self.dropped_count} æ¡, æ€»è®¡ {total_items} æ¡æŒ‡çº¹")
            except Exception as e:
                self.logger.warning(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
        # âœ… ä½¿ç”¨ResourceManagerç»Ÿä¸€æ¸…ç†
        import asyncio
        cleanup_result = await asyncio.to_thread(self._resource_manager.cleanup_all)
        
        if cleanup_result['success_count'] > 0:
            self.logger.info(
                f"Redisèµ„æºæ¸…ç†å®Œæˆ: æˆåŠŸ {cleanup_result['success_count']} ä¸ª"
            )
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)


# ================== ä½¿ç”¨ç¤ºä¾‹ ==================

if __name__ == '__main__':
    """
    ä½¿ç”¨ç¤ºä¾‹ï¼šåœ¨settings.pyä¸­é…ç½®
    
    ITEM_PIPELINES = {
        'examples.pipeline_migration_example.ImprovedMongoPipeline': 100,
        'examples.pipeline_migration_example.ImprovedCsvPipeline': 200,
        'examples.pipeline_migration_example.ImprovedRedisDedupPipeline': 300,
    }
    
    è¿è¡Œåå¯ä»¥è§‚å¯Ÿåˆ°ï¼š
    1. æ‰€æœ‰èµ„æºéƒ½ä¼šæ­£ç¡®æ¸…ç†
    2. æ‰¹é‡æ•°æ®ä¸ä¼šä¸¢å¤±
    3. å¼‚å¸¸æ—¶ä¹Ÿèƒ½ä¿è¯èµ„æºé‡Šæ”¾
    4. æ—¥å¿—ä¸­ä¼šæ˜¾ç¤ºè¯¦ç»†çš„æ¸…ç†ä¿¡æ¯
    """
    
    print("=" * 60)
    print("Pipelineèµ„æºç®¡ç†æ”¹é€ ç¤ºä¾‹")
    print("=" * 60)
    print()
    print("âœ… æ”¹è¿›ç‚¹ï¼š")
    print("  1. ä½¿ç”¨ResourceManagerç»Ÿä¸€ç®¡ç†èµ„æº")
    print("  2. LIFOæ¸…ç†é¡ºåºï¼Œç¡®ä¿ä¾èµ–å…³ç³»æ­£ç¡®")
    print("  3. å¼‚å¸¸å®¹é”™ï¼Œå•ä¸ªèµ„æºå¤±è´¥ä¸å½±å“å…¶ä»–")
    print("  4. å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…ä¸å¿…è¦çš„èµ„æºå ç”¨")
    print("  5. å®Œå–„çš„æ—¥å¿—è®°å½•ï¼Œä¾¿äºè°ƒè¯•")
    print()
    print("ğŸ“ ä½¿ç”¨æ–¹æ³•ï¼š")
    print("  1. å°†æ”¹è¿›çš„Pipelineç±»å¤åˆ¶åˆ°ä½ çš„é¡¹ç›®")
    print("  2. åœ¨settings.pyä¸­é…ç½®ITEM_PIPELINES")
    print("  3. è¿è¡Œçˆ¬è™«ï¼Œè§‚å¯Ÿèµ„æºæ¸…ç†æ—¥å¿—")
    print()
    print("ğŸ§ª éªŒè¯æ–¹æ³•ï¼š")
    print("  1. ä½¿ç”¨LeakDetectoræ£€æµ‹èµ„æºæ³„éœ²")
    print("  2. æŸ¥çœ‹æ—¥å¿—ä¸­çš„æ¸…ç†ä¿¡æ¯")
    print("  3. ä½¿ç”¨psutilç›‘æ§å†…å­˜å’Œæ–‡ä»¶æè¿°ç¬¦")
