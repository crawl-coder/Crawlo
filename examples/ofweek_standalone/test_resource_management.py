#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
åŸºäºofweek_standaloneé¡¹ç›®çš„èµ„æºç®¡ç†æµ‹è¯•
æµ‹è¯•åœºæ™¯ï¼š
1. å•æ¬¡è¿è¡Œ
2. è¿ç»­è¿è¡Œ3æ¬¡ï¼ˆæ£€æµ‹èµ„æºç´¯ç§¯ï¼‰
3. èµ„æºæ¸…ç†éªŒè¯
"""
import sys
import asyncio
import gc
import psutil
from pathlib import Path

# ç¡®ä¿èƒ½å¯¼å…¥crawloæ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from crawlo.crawler import CrawlerProcess
from crawlo.utils.leak_detector import LeakDetector


def get_memory_info():
    """è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä¿¡æ¯"""
    process = psutil.Process()
    memory_info = process.memory_info()
    return {
        'rss_mb': memory_info.rss / 1024 / 1024,
        'vms_mb': memory_info.vms / 1024 / 1024,
        'threads': process.num_threads(),
    }


def print_separator(title):
    """æ‰“å°åˆ†éš”çº¿"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80 + "\n")


async def test_single_run():
    """åœºæ™¯1ï¼šå•æ¬¡è¿è¡Œå•ä¸ªçˆ¬è™«"""
    print_separator("åœºæ™¯1ï¼šå•æ¬¡è¿è¡Œå•ä¸ªçˆ¬è™«")
    
    detector = LeakDetector(name="single_run")
    detector.set_baseline("å¯åŠ¨å‰")
    
    before = get_memory_info()
    print(f"è¿è¡Œå‰: RSS={before['rss_mb']:.2f}MB, çº¿ç¨‹={before['threads']}")
    
    try:
        process = CrawlerProcess()
        await process.crawl('of_week_standalone')
        
        detector.snapshot("çˆ¬è™«è¿è¡Œå")
        
        # åƒåœ¾å›æ”¶
        gc.collect()
        await asyncio.sleep(0.5)
        
        detector.snapshot("åƒåœ¾å›æ”¶å")
        
    except Exception as e:
        print(f"âŒ åœºæ™¯1å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    after = get_memory_info()
    print(f"è¿è¡Œå: RSS={after['rss_mb']:.2f}MB, çº¿ç¨‹={after['threads']}")
    
    # åˆ†æèµ„æº
    analysis = detector.analyze(threshold_mb=30.0)
    print(f"\nğŸ“Š èµ„æºå˜åŒ–:")
    changes = analysis.get('changes', {})
    print(f"  å†…å­˜å¢é•¿: {changes.get('memory_mb', 0):.2f} MB")
    print(f"  å†…å­˜ç™¾åˆ†æ¯”: {changes.get('memory_percent', 0):.1f}%")
    print(f"  å¯¹è±¡æ•°å˜åŒ–: {changes.get('object_count', 0):+d}")
    print(f"  çº¿ç¨‹æ•°å˜åŒ–: {changes.get('thread_count', 0):+d}")
    
    if analysis['potential_leaks']:
        print(f"\nâš ï¸  æ£€æµ‹åˆ° {len(analysis['potential_leaks'])} ä¸ªæ½œåœ¨é—®é¢˜:")
        for leak in analysis['potential_leaks']:
            severity = leak.get('severity', 'unknown')
            leak_type = leak.get('type', 'unknown')
            growth = leak.get('growth_mb', leak.get('growth', 0))
            print(f"  - {leak_type}: {growth} (ä¸¥é‡ç¨‹åº¦: {severity})")
        return False
    else:
        print("âœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„èµ„æºæ³„éœ²")
        return True


async def test_continuous_runs():
    """åœºæ™¯2ï¼šè¿ç»­è¿è¡Œ3æ¬¡çˆ¬è™«"""
    print_separator("åœºæ™¯2ï¼šè¿ç»­è¿è¡Œ3æ¬¡çˆ¬è™«ï¼ˆæ£€æµ‹èµ„æºç´¯ç§¯ï¼‰")
    
    detector = LeakDetector(name="continuous_runs")
    detector.set_baseline("åˆå§‹çŠ¶æ€")
    
    memory_records = []
    
    for i in range(3):
        print(f"\n--- ç¬¬ {i+1} æ¬¡è¿è¡Œ ---")
        
        before = get_memory_info()
        memory_records.append(before['rss_mb'])
        print(f"è¿è¡Œå‰: {before['rss_mb']:.2f}MB")
        
        try:
            process = CrawlerProcess()
            await process.crawl('of_week_standalone')
            
            detector.snapshot(f"ç¬¬{i+1}æ¬¡è¿è¡Œå")
            
            # åƒåœ¾å›æ”¶
            gc.collect()
            await asyncio.sleep(1)
            
        except Exception as e:
            print(f"âŒ ç¬¬{i+1}æ¬¡è¿è¡Œå¤±è´¥: {e}")
            return False
        
        after = get_memory_info()
        print(f"è¿è¡Œå: {after['rss_mb']:.2f}MB (+{after['rss_mb']-before['rss_mb']:.2f}MB)")
    
    # åˆ†æè¶‹åŠ¿
    print(f"\nğŸ“Š å†…å­˜å˜åŒ–è¶‹åŠ¿:")
    for i, mem in enumerate(memory_records, 1):
        print(f"  ç¬¬{i}æ¬¡: {mem:.2f}MB")
    
    if len(memory_records) >= 3:
        growth_1_2 = memory_records[1] - memory_records[0]
        growth_2_3 = memory_records[2] - memory_records[1]
        
        print(f"\nå¢é•¿åˆ†æ:")
        print(f"  ç¬¬1â†’2æ¬¡: +{growth_1_2:.2f}MB")
        print(f"  ç¬¬2â†’3æ¬¡: +{growth_2_3:.2f}MB")
        
        if growth_2_3 > 30:
            print("âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°æŒç»­çš„å†…å­˜å¢é•¿")
            return False
        else:
            print("âœ… å†…å­˜å¢é•¿è¶‹äºç¨³å®š")
            return True
    
    return True


async def test_resource_cleanup():
    """åœºæ™¯3ï¼šéªŒè¯èµ„æºæ¸…ç†"""
    print_separator("åœºæ™¯3ï¼šéªŒè¯èµ„æºæ¸…ç†å®Œæ•´æ€§")
    
    detector = LeakDetector(name="cleanup_test")
    detector.set_baseline("å¯åŠ¨å‰")
    
    try:
        process = CrawlerProcess()
        
        # æ£€æŸ¥èµ„æºç®¡ç†å™¨é›†æˆ
        crawler = getattr(process, '_crawler', None)
        if crawler and hasattr(crawler, '_resource_manager'):
            print("âœ… å·²é›†æˆ ResourceManager")
            rm = crawler._resource_manager  # type: ignore
            if hasattr(rm, 'get_registered_count'):
                count = rm.get_registered_count()
                print(f"ğŸ“‹ æ³¨å†Œèµ„æºæ•°: {count}")
        else:
            print("âš ï¸  æœªé›†æˆ ResourceManager")
        
        await process.crawl('of_week_standalone')
        
        detector.snapshot("è¿è¡Œå")
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        await asyncio.sleep(0.5)
        
        detector.snapshot("æ¸…ç†å")
        
    except Exception as e:
        print(f"âŒ åœºæ™¯3å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # åˆ†ææ¸…ç†æ•ˆæœ
    analysis = detector.analyze(threshold_mb=30.0)
    
    print(f"\nğŸ“Š æ¸…ç†æ•ˆæœ:")
    changes = analysis.get('changes', {})
    print(f"  å†…å­˜å˜åŒ–: {changes.get('memory_mb', 0):.2f} MB")
    print(f"  å¯¹è±¡æ•°å˜åŒ–: {changes.get('object_count', 0):+d}")
    
    if analysis['potential_leaks']:
        print(f"\nâš ï¸  å‘ç° {len(analysis['potential_leaks'])} ä¸ªæ¸…ç†é—®é¢˜")
        return False
    else:
        print("âœ… èµ„æºæ¸…ç†å®Œæ•´")
        return True


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ èµ„æºç®¡ç†æµ‹è¯• - ofweek_standaloneé¡¹ç›®")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    results = {}
    
    # åœºæ™¯1
    results['åœºæ™¯1'] = await test_single_run()
    await asyncio.sleep(2)
    gc.collect()
    
    # åœºæ™¯2
    results['åœºæ™¯2'] = await test_continuous_runs()
    await asyncio.sleep(2)
    gc.collect()
    
    # åœºæ™¯3
    results['åœºæ™¯3'] = await test_resource_cleanup()
    
    # æ±‡æ€»
    print_separator("æµ‹è¯•ç»“æœæ±‡æ€»")
    
    all_passed = True
    for scenario, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{scenario}: {status}")
        if not passed:
            all_passed = False
    
    print("\n" + "=" * 80)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼èµ„æºç®¡ç†åŠŸèƒ½æ­£å¸¸ã€‚")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
        return 1


if __name__ == '__main__':
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
