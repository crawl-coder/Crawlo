# -*- coding: utf-8 -*-
"""
===================================
é›†æˆé€šçŸ¥åŠŸèƒ½çš„ ofweek çˆ¬è™«ç¤ºä¾‹
===================================

å±•ç¤ºå¦‚ä½•åœ¨å®é™…çˆ¬è™«ä¸­é›†æˆ Crawlo é€šçŸ¥ç³»ç»Ÿ
"""

from crawlo.spider import Spider
from crawlo import Request, Response
from ..items import OfWeekStandaloneItem
from crawlo.bot.handlers import send_crawler_status, send_crawler_alert, send_crawler_progress
from crawlo.bot.models import ChannelType
import asyncio


class OfWeekSpiderWithNotifications(Spider):
    """é›†æˆé€šçŸ¥åŠŸèƒ½çš„ ofweek çˆ¬è™«"""
    
    name = 'of_week_with_notifications'
    allowed_domains = ['ee.ofweek.com']
    start_urls = ['https://ee.ofweek.com/']
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.stats = {
            'total_requests': 0,
            'successful_items': 0,
            'failed_requests': 0,
            'start_time': None
        }
    
    async def start_requests(self):
        """ç”Ÿæˆåˆå§‹è¯·æ±‚ - å¸¦å¯åŠ¨é€šçŸ¥"""
        # å‘é€çˆ¬è™«å¯åŠ¨é€šçŸ¥
        await send_crawler_status(
            title="ã€å¯åŠ¨ã€‘ofweekçˆ¬è™«å¼€å§‹è¿è¡Œ",
            content=f"çˆ¬è™«ä»»åŠ¡ '{self.name}' å·²å¯åŠ¨ï¼Œå¼€å§‹æŠ“å– ofweek æ–°é—»æ•°æ®...",
            channel=ChannelType.DINGTALK
        )
        
        self.stats['start_time'] = self.get_current_time()
        self.logger.info("çˆ¬è™«å¯åŠ¨é€šçŸ¥å·²å‘é€")
        
        # åŸæœ‰çš„èµ·å§‹è¯·æ±‚é€»è¾‘
        max_pages = 10
        start_urls = []
        for page in range(1, max_pages + 1):
            url = f'https://ee.ofweek.com/CATList-2800-8100-ee-{page}.html'
            start_urls.append(url)
        
        self.logger.info(f"ç”Ÿæˆäº† {len(start_urls)} ä¸ªèµ·å§‹URL")
        
        for url in start_urls:
            self.stats['total_requests'] += 1
            yield Request(url, callback=self.parse, dont_filter=True)
    
    async def parse(self, response: Response):
        """è§£æå“åº” - å¸¦è¿›åº¦å’Œå¼‚å¸¸é€šçŸ¥"""
        try:
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                self.stats['failed_requests'] += 1
                error_msg = f"é¡µé¢è¿”å›é200çŠ¶æ€ç : {response.status_code}"
                self.logger.warning(f"{error_msg}, URL: {response.url}")
                
                # å‘é€å‘Šè­¦é€šçŸ¥
                await send_crawler_alert(
                    title="ã€å‘Šè­¦ã€‘é¡µé¢è®¿é—®å¤±è´¥",
                    content=f"URL: {response.url}\nçŠ¶æ€ç : {response.status_code}\nå·²è®°å½•å¹¶ç»§ç»­å¤„ç†å…¶ä»–è¯·æ±‚",
                    channel=ChannelType.DINGTALK
                )
                return
            
            # æ£€æŸ¥é¡µé¢å†…å®¹æ˜¯å¦ä¸ºç©º
            if not response.text or len(response.text.strip()) == 0:
                self.stats['failed_requests'] += 1
                self.logger.warning(f"é¡µé¢å†…å®¹ä¸ºç©º: {response.url}")
                return
            
            # æ•°æ®æå–
            rows = response.xpath('//div[@class="main_left"]/div[@class="list_model"]/div[@class="model_right model_right2"]')
            self.logger.info(f"åœ¨é¡µé¢ {response.url} ä¸­æ‰¾åˆ° {len(rows)} ä¸ªæ¡ç›®")
            
            # å‘é€è¿›åº¦é€šçŸ¥ï¼ˆæ¯å¤„ç†5ä¸ªé¡µé¢å‘é€ä¸€æ¬¡ï¼‰
            if self.stats['total_requests'] % 5 == 0:
                await send_crawler_progress(
                    title="ã€è¿›åº¦ã€‘æ•°æ®æŠ“å–è¿›åº¦",
                    content=f"å·²å¤„ç† {self.stats['total_requests']} ä¸ªé¡µé¢ï¼ŒæˆåŠŸæå– {len(rows)} æ¡æ•°æ®",
                    channel=ChannelType.DINGTALK
                )
            
            for row in rows:
                try:
                    # æå–URLå’Œæ ‡é¢˜
                    url = row.xpath('./h3/a/@href').extract_first()
                    title = row.xpath('./h3/a/text()').extract_first()
                    
                    # å®¹é”™å¤„ç†
                    if not url or not title:
                        continue
                    
                    # ç¡®ä¿ URL æ˜¯ç»å¯¹è·¯å¾„
                    absolute_url = response.urljoin(url)
                    
                    # éªŒè¯URLæ ¼å¼
                    if not absolute_url.startswith(('http://', 'https://')):
                        continue
                    
                    yield Request(
                        url=absolute_url,
                        meta={
                            "title": title.strip(),
                            "parent_url": response.url
                        },
                        callback=self.parse_detail
                    )
                    
                except Exception as e:
                    self.logger.error(f"å¤„ç†æ¡ç›®æ—¶å‡ºé”™: {e}")
                    continue
                    
        except Exception as e:
            self.stats['failed_requests'] += 1
            error_msg = f"è§£æé¡µé¢æ—¶å‡ºé”™: {str(e)}"
            self.logger.error(error_msg)
            
            # å‘é€ä¸¥é‡é”™è¯¯å‘Šè­¦
            await send_crawler_alert(
                title="ã€ä¸¥é‡å‘Šè­¦ã€‘é¡µé¢è§£æå¼‚å¸¸",
                content=f"URL: {response.url}\né”™è¯¯ä¿¡æ¯: {error_msg}\nè¯·æ£€æŸ¥é¡µé¢ç»“æ„æ˜¯å¦å‘ç”Ÿå˜åŒ–",
                channel=ChannelType.DINGTALK
            )
    
    async def parse_detail(self, response):
        """è§£æè¯¦æƒ…é¡µé¢ - å¸¦æ•°æ®ç»Ÿè®¡é€šçŸ¥"""
        try:
            self.logger.info(f'æ­£åœ¨è§£æè¯¦æƒ…é¡µ: {response.url}')
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                self.stats['failed_requests'] += 1
                self.logger.warning(f"è¯¦æƒ…é¡µè¿”å›é200çŠ¶æ€ç : {response.status_code}")
                return
            
            title = response.meta.get('title', '')
            
            # æå–å†…å®¹
            content_elements = response.xpath('//div[@class="TRS_Editor"]|//*[@id="articleC"]')
            if content_elements:
                content = content_elements.xpath('.//text()').extract()
                content = '\n'.join([text.strip() for text in content if text.strip()])
            else:
                content = ''
            
            # æå–å‘å¸ƒæ—¶é—´å’Œæ¥æº
            publish_time = response.xpath('//div[@class="time fl"]/text()').extract_first()
            source = response.xpath('//div[@class="source-name"]/text()').extract_first()
            
            # åˆ›å»ºæ•°æ®é¡¹
            item = OfWeekStandaloneItem()
            item['title'] = title.strip() if title else ''
            item['publish_time'] = publish_time.strip() if publish_time else ''
            item['url'] = response.url
            item['source'] = source.strip() if source else ''
            item['content'] = content
            
            self.stats['successful_items'] += 1
            
            # æ¯æˆåŠŸå¤„ç†100æ¡æ•°æ®å‘é€ä¸€æ¬¡è¿›åº¦é€šçŸ¥
            if self.stats['successful_items'] % 100 == 0:
                await send_crawler_progress(
                    title="ã€æ•°æ®ç»Ÿè®¡ã€‘æŠ“å–è¿›åº¦æ›´æ–°",
                    content=f"ç´¯è®¡æˆåŠŸæŠ“å– {self.stats['successful_items']} æ¡æ•°æ®\nå¤±è´¥è¯·æ±‚: {self.stats['failed_requests']} æ¬¡",
                    channel=ChannelType.DINGTALK
                )
            
            yield item
            
        except Exception as e:
            self.stats['failed_requests'] += 1
            self.logger.error(f"è§£æè¯¦æƒ…é¡µ {response.url} æ—¶å‡ºé”™: {e}")
    
    async def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶çš„å›è°ƒ - å‘é€æ€»ç»“é€šçŸ¥"""
        # è®¡ç®—è¿è¡Œæ—¶é•¿
        run_duration = self.get_run_duration()
        
        # å‘é€ä»»åŠ¡å®Œæˆæ€»ç»“é€šçŸ¥
        await send_crawler_status(
            title="ã€å®Œæˆã€‘ofweekçˆ¬è™«ä»»åŠ¡æ€»ç»“",
            content=f"""çˆ¬è™«ä»»åŠ¡ '{self.name}' å·²å®Œæˆï¼
ğŸ“Š è¿è¡Œç»Ÿè®¡ï¼š
   â€¢ æ€»è¯·æ±‚æ•°: {self.stats['total_requests']}
   â€¢ æˆåŠŸæŠ“å–: {self.stats['successful_items']} æ¡æ•°æ®
   â€¢ å¤±è´¥è¯·æ±‚: {self.stats['failed_requests']} æ¬¡
   â€¢ è¿è¡Œæ—¶é•¿: {run_duration}
âœ… æ•°æ®å·²å­˜å‚¨åˆ° MySQL æ•°æ®åº“
ğŸ“ é¡¹ç›®: ofweek_standalone""",
            channel=ChannelType.DINGTALK
        )
        
        self.logger.info("çˆ¬è™«å®Œæˆæ€»ç»“é€šçŸ¥å·²å‘é€")
    
    def get_current_time(self):
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime
        return datetime.now()
    
    def get_run_duration(self):
        """è®¡ç®—è¿è¡Œæ—¶é•¿"""
        if not self.stats['start_time']:
            return "æœªçŸ¥"
        
        from datetime import datetime
        duration = datetime.now() - self.stats['start_time']
        hours, remainder = divmod(duration.seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’"


# ä½¿ç”¨ç¤ºä¾‹
def run_spider_with_notifications():
    """è¿è¡Œå¸¦é€šçŸ¥çš„çˆ¬è™«ç¤ºä¾‹"""
    print("ğŸš€ å¯åŠ¨é›†æˆé€šçŸ¥åŠŸèƒ½çš„ ofweek çˆ¬è™«...")
    
    # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„çˆ¬è™«è¿è¡Œä»£ç 
    # ç”±äºè¿™æ˜¯ç¤ºä¾‹ï¼Œæˆ‘ä»¬åªæ¼”ç¤ºé€šçŸ¥åŠŸèƒ½
    
    async def demo():
        # æ¨¡æ‹Ÿçˆ¬è™«è¿è¡Œè¿‡ç¨‹ä¸­çš„é€šçŸ¥
        await send_crawler_status(
            title="ã€ç¤ºä¾‹ã€‘çˆ¬è™«é€šçŸ¥åŠŸèƒ½æ¼”ç¤º",
            content="è¿™æ˜¯æ¼”ç¤ºå¦‚ä½•åœ¨çˆ¬è™«ä¸­ä½¿ç”¨é€šçŸ¥åŠŸèƒ½çš„ç¤ºä¾‹",
            channel=ChannelType.DINGTALK
        )
    
    asyncio.run(demo())
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    run_spider_with_notifications()